{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0340d20a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "本篇文章主要介绍如何使用 Tensorbay 和 Tensorflow 进行简单的文本分类。\n",
    "\n",
    "我们首先介绍 Tensorbay 相关使用，然后简单介绍 NLP 领域常用的分类模型：TextCNN，最后将所有的串起来完成一个简单的文本分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60cfad1",
   "metadata": {},
   "source": [
    "## TensorBay\n",
    "\n",
    "TensorBay 是什么？借用官方的介绍：\n",
    "\n",
    ">作为非结构化数据管理专家，TensorBay 提供数据托管、复杂数据版本管理、在线数据可视化和数据协作等服务。 TensorBay 的统一权限管理，让您的数据共享和协同使用更加安全。\n",
    "\n",
    "简单来说，它提供了统一的数据集管理方案。官方提供了 Python SDK，用于：\n",
    "\n",
    "- 通过 Pythonic 的方式访问数据资源\n",
    "- 易于使用的 CLI 工具\n",
    "- 统一的数据集结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a257820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地与 TensorBay 服务器交互的客户端\n",
    "from tensorbay import GAS\n",
    "# Segment 用于分割数据集的不同部分\n",
    "from tensorbay.dataset import Segment\n",
    "# Dataset 数据集工具，包含一系列的 Segment\n",
    "from tensorbay.dataset import Dataset as TensorBayDataset\n",
    "# Note 信息\n",
    "from tensorbay.dataset import Notes\n",
    "# 内置的 Newsgroup20 数据集\n",
    "from tensorbay.opendataset import Newsgroups20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268c4e5",
   "metadata": {},
   "source": [
    "### GAS\n",
    "\n",
    "首先，需要在官方网站注册一个账号：https://gas.graviti.cn/tensorbay/\n",
    "\n",
    "登陆后，在【开发者工具】里面的【AccessKey】中新建一个 Key，就可以用于连接 TensorBay 了。\n",
    "\n",
    "创建 GAS 后，之后的操作就都基于该验证进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9e8b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = \"Accesskey-098e0c26fdc79f31a085f5b897052ba4\" #\"<YOUR_ACCESS_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72cf3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas = GAS(access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41bd711",
   "metadata": {},
   "source": [
    "### TensorBayDataset\n",
    "\n",
    "一般会在此定义数据集，当然官方也内置了一些数据集。数据集结构如下：\n",
    "\n",
    "```bash\n",
    "dataset\n",
    "├── notes\n",
    "├── catalog\n",
    "│   ├── subcatalog\n",
    "│   ├── subcatalog\n",
    "│   └── ...\n",
    "├── segment\n",
    "│   ├── data\n",
    "│   ├── data\n",
    "│   └── ...\n",
    "├── segment\n",
    "│   ├── data\n",
    "│   ├── data\n",
    "│   └── ...\n",
    "└── ...\n",
    "```\n",
    "\n",
    "- dataset：最顶层的概念，每个数据集包括一个目录和一些 Segment，对应的 class 是 `Dataset`\n",
    "- notes：包含一个数据集的基本信息，对应的 class 是 `Notes`，包括：\n",
    "  - 数据集中数据的时间连续性\n",
    "  - 数据集中的 bin 点云文件的字段\n",
    "- catalog：用于存储标签元信息，它收集与数据集对应的所有标签，一个目录下可以有一个或多个子目录（标签格式），每个子目录只存储一种标签类型的标签元信息，包括对应的注解是否有跟踪信息。**注意：如果没有标签信息，则不需要 catalog**\n",
    "\n",
    "  以 BSTLD 数据集（Box2D 为 label）为例，它的 catalog 如下所示：\n",
    "  ```json\n",
    "  {\n",
    "    \"BOX2D\": {\n",
    "        \"categories\": [\n",
    "            { \"name\": \"Red\" },\n",
    "            { \"name\": \"RedLeft\" },\n",
    "            { \"name\": \"RedRight\" },\n",
    "            { \"name\": \"RedStraight\" },\n",
    "            { \"name\": \"RedStraightLeft\" },\n",
    "            { \"name\": \"Green\" },\n",
    "            { \"name\": \"GreenLeft\" },\n",
    "            { \"name\": \"GreenRight\" },\n",
    "            { \"name\": \"GreenStraight\" },\n",
    "            { \"name\": \"GreenStraightLeft\" },\n",
    "            { \"name\": \"GreenStraigntRight\" },\n",
    "            { \"name\": \"Yellow\" },\n",
    "            { \"name\": \"off\" }\n",
    "        ],\n",
    "        \"attributes\": [\n",
    "            {\n",
    "                \"name\": \"occluded\",\n",
    "                \"type\": \"boolean\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "  ```\n",
    "- segment：数据集的不同部分，每个部分存在一个 Segment 中，比如训练集、测试集，对应的 class 是 `Segment`\n",
    "- data：一个 Data 包含一个数据集样本和对应的标签，以及其他任何信息（如时间戳），对应的 class 是 `Data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ba5b8",
   "metadata": {},
   "source": [
    "### 内置数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b2ac8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载一个内置的数据集\n",
    "news = TensorBayDataset(\"Newsgroups20_public\", gas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b947b8b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(\"Newsgroups20\") [\n",
      "  Segment(\"20_newsgroups\") [...],\n",
      "  Segment(\"20news-18828\") [...],\n",
      "  Segment(\"20news-bydate-test\") [...],\n",
      "  Segment(\"20news-bydate-train\") [...]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 包括四个 Segment\n",
    "print(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8ac90a",
   "metadata": {},
   "source": [
    "四个 Segment 分别表示：\n",
    "\n",
    "- `20_newsgroups`：Ver1.0，原始的 20 Newsgroups 数据集\n",
    "- `20news-bydate-*`：Ver2.0，根据 date 排序，并删除了重复项和一些标题\n",
    "- `20news-18828`：Ver3.0，只包括发件人和主题两个标题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6824136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameList [\n",
      "  CategoryInfo(\"alt.atheism\"),\n",
      "  CategoryInfo(\"comp.graphics\"),\n",
      "  CategoryInfo(\"comp.os\"),\n",
      "  CategoryInfo(\"comp.sys\"),\n",
      "  CategoryInfo(\"comp.windows\"),\n",
      "  CategoryInfo(\"misc.forsale\"),\n",
      "  CategoryInfo(\"rec.autos\"),\n",
      "  CategoryInfo(\"rec.motorcycles\"),\n",
      "  CategoryInfo(\"rec.sport\"),\n",
      "  CategoryInfo(\"sci.crypt\"),\n",
      "  CategoryInfo(\"sci.electronics\"),\n",
      "  CategoryInfo(\"sci.med\"),\n",
      "  CategoryInfo(\"sci.space\"),\n",
      "  CategoryInfo(\"soc.religion\"),\n",
      "  CategoryInfo(\"talk.politics\"),\n",
      "  CategoryInfo(\"talk.religion\")\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 分类的类型，为啥只有 16 个？？？\n",
    "print(news.catalog.classification.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3d81c",
   "metadata": {},
   "source": [
    "### 本地数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b14aaf9",
   "metadata": {},
   "source": [
    "或者，也可以使用自定义的数据集，比如我们把 20news-18828 数据集下载到本地：\n",
    "\n",
    "```bash\n",
    "tree . -L 2\n",
    ".\n",
    "└── 20news-18828\n",
    "    ├── alt.atheism\n",
    "    ├── comp.graphics\n",
    "    ├── comp.os.ms-windows.misc\n",
    "    ├── comp.sys.ibm.pc.hardware\n",
    "    ├── comp.sys.mac.hardware\n",
    "    ├── comp.windows.x\n",
    "    ├── misc.forsale\n",
    "    ├── rec.autos\n",
    "    ├── rec.motorcycles\n",
    "    ├── rec.sport.baseball\n",
    "    ├── rec.sport.hockey\n",
    "    ├── sci.crypt\n",
    "    ├── sci.electronics\n",
    "    ├── sci.med\n",
    "    ├── sci.space\n",
    "    ├── soc.religion.christian\n",
    "    ├── talk.politics.guns\n",
    "    ├── talk.politics.mideast\n",
    "    ├── talk.politics.misc\n",
    "    └── talk.religion.misc\n",
    "\n",
    "21 directories, 0 files\n",
    "\n",
    "```\n",
    "\n",
    "正好 20 个类别，看来内置的有点问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "902a1da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: bd@fluent@dartmouth.EDU (Brice Dowaliby)\r\n",
      "Subject: Re: Who's next?  Mormons and Jews?\r\n",
      "\r\n",
      "dic5340@hertz.njit.edu (David Charlap) writes:\r\n",
      "\r\n",
      ">Someone in the government actually believed Koresh knew the \"seven\r\n",
      ">seals of the apocalypse\", and ordered the invasion so that they'd all\r\n",
      ">be dead and unable to talk about them in public.\r\n",
      "\r\n",
      "Everything we need to know about the seven seals is already\r\n",
      "in the bible.  There is no \"knowledge\" of the seals that\r\n",
      "Koresh could have.\r\n",
      "\r\n",
      "Unless the FBI were to kill all publishers of the bible, it\r\n",
      "would seem the story of the seven seals would be bound to\r\n",
      "leak out.\r\n",
      "\r\n",
      "Assuming for the moment that the FBI believed in the bible and \r\n",
      "were afraid of the seven seals, then they would also know\r\n",
      "that God is the one who has to open the seals, not some\r\n"
     ]
    }
   ],
   "source": [
    "# 查看样本，一个文件对应一个样本\n",
    "!head -20 data/20news-18828/talk.religion.misc/84277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec20e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接把 data 目录加载进来\n",
    "ds = Newsgroups20(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8757aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(\"Newsgroups20\") [\n",
      "  Segment(\"20news-18828\") [...]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "279d269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameList [\n",
      "  CategoryInfo(\"alt.atheism\"),\n",
      "  CategoryInfo(\"comp.graphics\"),\n",
      "  CategoryInfo(\"comp.os.ms-windows.misc\"),\n",
      "  CategoryInfo(\"comp.sys.ibm.pc.hardware\"),\n",
      "  CategoryInfo(\"comp.sys.mac.hardware\"),\n",
      "  CategoryInfo(\"comp.windows.x\"),\n",
      "  CategoryInfo(\"misc.forsale\"),\n",
      "  CategoryInfo(\"rec.autos\"),\n",
      "  CategoryInfo(\"rec.motorcycles\"),\n",
      "  CategoryInfo(\"rec.sport.baseball\"),\n",
      "  CategoryInfo(\"rec.sport.hockey\"),\n",
      "  CategoryInfo(\"sci.crypt\"),\n",
      "  CategoryInfo(\"sci.electronics\"),\n",
      "  CategoryInfo(\"sci.med\"),\n",
      "  CategoryInfo(\"sci.space\"),\n",
      "  CategoryInfo(\"soc.religion.christian\"),\n",
      "  CategoryInfo(\"talk.politics.guns\"),\n",
      "  CategoryInfo(\"talk.politics.mideast\"),\n",
      "  CategoryInfo(\"talk.politics.misc\"),\n",
      "  CategoryInfo(\"talk.religion.misc\")\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 正好 20 个\n",
    "print(ds.catalog.classification.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea3add",
   "metadata": {},
   "source": [
    "这里，只需要将数据集按指定的格式和目录放置即可，要求如下：\n",
    "\n",
    "```bash\n",
    "<path>\n",
    "    20news-18828/\n",
    "        alt.atheism/\n",
    "            49960\n",
    "            51060\n",
    "            51119\n",
    "            51120\n",
    "            ...\n",
    "        comp.graphics/\n",
    "        comp.os.ms-windows.misc/\n",
    "        comp.sys.ibm.pc.hardware/\n",
    "        comp.sys.mac.hardware/\n",
    "        comp.windows.x/\n",
    "        misc.forsale/\n",
    "        rec.autos/\n",
    "        rec.motorcycles/\n",
    "        rec.sport.baseball/\n",
    "        rec.sport.hockey/\n",
    "        sci.crypt/\n",
    "        sci.electronics/\n",
    "        sci.med/\n",
    "        sci.space/\n",
    "        soc.religion.christian/\n",
    "        talk.politics.guns/\n",
    "        talk.politics.mideast/\n",
    "        talk.politics.misc/\n",
    "        talk.religion.misc/\n",
    "    20news-bydate-test/\n",
    "    20news-bydate-train/\n",
    "    20_newsgroups/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85629ac2",
   "metadata": {},
   "source": [
    "### Segment\n",
    "\n",
    "Segment 是数据集的不同部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3b777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内置\n",
    "seg_name = \"20news-18828\"\n",
    "dataset_client = gas.get_dataset(\"Newsgroups20\")\n",
    "segment = Segment(seg_name, dataset_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "401de8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segment(\"20news-18828\") [\n",
       "  RemoteData(\"alt.atheism_49960.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51060.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51119.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51120.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51121.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51122.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51123.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51124.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51125.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51126.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51127.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51128.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51130.txt\")(...),\n",
       "  RemoteData(\"alt.atheism_51131.txt\")(...),\n",
       "  ... (18813 items are folded),\n",
       "  RemoteData(\"talk.religion.misc_84570.txt\")(...)\n",
       "]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e0b6b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoteData(\"alt.atheism_49960.txt\")(\n",
      "  (label): Label(\n",
      "    (classification): Classification(\n",
      "      (category): 'alt.atheism'\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(segment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1d11dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(\"/Users/Yam/Yam/All4NLP/Tensorbay/data/20news-18828/alt.atheism/49960\")(\n",
      "  (label): Label(\n",
      "    (classification): Classification(\n",
      "      (category): 'alt.atheism'\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 本地\n",
    "seg = ds[\"20news-18828\"]\n",
    "print(seg[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d07a8",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9297b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个样本\n",
    "ele = seg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00f0d6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorbay.dataset.data.Data'>\n"
     ]
    }
   ],
   "source": [
    "print(type(ele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1880ac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label(\n",
      "  (classification): Classification(\n",
      "    (category): 'alt.atheism'\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 标签\n",
    "print(ele.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8d1b2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'From: mathew <mathew@mantis.co.uk>\\nSubject: Alt.Atheism FAQ: Atheist Resources\\n\\nArchive-name: atheism/resources\\nAlt-atheism-archive-name: resources\\nLast-modified: 11 December 1992\\nVersion: 1.0\\n\\n                              Atheist Resources\\n\\n                      Addresses of Atheist Organizations\\n\\n                                     USA\\n\\nFREEDOM FROM RELIGION FOUNDATION\\n\\nDarwin fish bumper stickers and assorted other atheist paraphernalia are\\navailable from the Freedom From Religion Foundatio'\n"
     ]
    }
   ],
   "source": [
    "# 数据\n",
    "print(ele.open().read()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "277d0df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18828\n"
     ]
    }
   ],
   "source": [
    "# 遍历数据集\n",
    "# 每个 v 都是一个 ele 的 Data 实例\n",
    "i = 0\n",
    "for v in seg:\n",
    "    i += 1\n",
    "    pass\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b76014d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorbay.dataset.data.Data'>\n"
     ]
    }
   ],
   "source": [
    "print(type(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d485871b",
   "metadata": {},
   "source": [
    "## TextCNN\n",
    "\n",
    "TextCNN 是 CNN 的 NLP 版本，来自 Kim 的 [[1408.5882] Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)\n",
    "\n",
    "结构如下：\n",
    "\n",
    "![](http://qnimg.lovevivian.cn/paper-textcnn-1.jpg)\n",
    "\n",
    "大致原理是使用多个不同大小的 filter（也叫 kernel） 对文本进行特征提取，如上图所示：\n",
    "\n",
    "- 首先通过 Embedding 将输入的句子映射为一个 `n_seq * embed_size` 大小的张量（实际中一般还会有 batch_size）\n",
    "- 使用 `(filter_size, embed_size)` 大小的 filter 在输入句子序列上平滑移动，这里使用不同的 padding 策略，会得到不同 size 的输出\n",
    "- 由于有 `num_filters` 个输出通道，所以上面的输出会有 `num_filters` 个\n",
    "- 使用 Max Pooling 或 Average Pooling，沿着序列方向得到结果，最终每个 filter 的输出 size 为 `num_filters`\n",
    "- 将不同 filter 的输出拼接后展开，作为句子的表征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b53a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras as tfk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf66adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cnn(config, inputs):\n",
    "    embed = tfk.layers.Embedding(config.vocab_size, config.embed_size, input_length=config.max_len)(inputs)\n",
    "    embed = embed[:, :, :, None]\n",
    "    pool_outputs = []\n",
    "    for filter_size in map(int, config.filter_sizes.split(',')):\n",
    "        # 卷积\n",
    "        conv = tfk.layers.Conv2D(\n",
    "            config.num_filters, \n",
    "            kernel_size=(filter_size, config.embed_size), \n",
    "            strides=(1, 1), \n",
    "            padding='valid')(embed)\n",
    "        # Max Pooling\n",
    "        pool = tfk.layers.MaxPool2D(\n",
    "            pool_size=(config.max_len - filter_size + 1, 1), \n",
    "            strides=(1, 1), \n",
    "            padding='valid')(conv)\n",
    "        pool_outputs.append(pool)\n",
    "\n",
    "    z = tfk.layers.concatenate(pool_outputs, axis=-1)\n",
    "    z = tf.squeeze(z, [1, 2])\n",
    "    z = tfk.layers.Dropout(config.dropout)(z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b277730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pnlp import Dict\n",
    "config = Dict({\n",
    "    \"vocab_size\": 21128,\n",
    "    \"embed_size\": 256,\n",
    "    \"max_len\": 512,\n",
    "    \"num_filters\": 128,\n",
    "    \"filter_sizes\": \"2,3,4\",\n",
    "    \"dropout\": 0.1,\n",
    "    \"num_classes\": 20\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "437621c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = text_cnn(config, tf.constant([[1]*512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "291126c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 384)\n"
     ]
    }
   ],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688cd6a7",
   "metadata": {},
   "source": [
    "## AllTogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6062cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "import pnlp\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e604c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGroupSegment:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        client: Union[str, GAS], \n",
    "        segment_name: str, \n",
    "        tokenizer_path: str, \n",
    "        label_file: str, \n",
    "        max_length: int = 512\n",
    "    ):\n",
    "        # 使用内置数据\n",
    "        if isinstance(client, GAS):\n",
    "            self.dataset = TensorBayDataset(\"Newsgroups20\", client)\n",
    "        # 使用本地数据\n",
    "        elif isinstance(client, str):\n",
    "            self.dataset = Newsgroups20(client)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dataset client\")\n",
    "        self.segment = self.dataset[segment_name]\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        labels = pnlp.read_lines(label_file)\n",
    "        self.category_to_index = dict(zip(labels, range(len(labels))))\n",
    "\n",
    "    def __call__(self):\n",
    "        for data in self.segment:\n",
    "            with data.open() as fp:\n",
    "                txt = fp.read().decode(\"utf8\", errors=\"ignore\")\n",
    "                # tokenize\n",
    "                ids = self.tokenizer.encode(\n",
    "                    txt, max_length=self.max_length, truncation=True, padding=\"max_length\"\n",
    "                )\n",
    "                input_tensor = tf.convert_to_tensor(np.array(ids), dtype=tf.int32)\n",
    "            category = self.category_to_index[data.label.classification.category]\n",
    "            category_tensor = tf.convert_to_tensor(category, dtype=tf.int32)\n",
    "            yield input_tensor, category_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "261df0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, module):\n",
    "    inputs = tfk.Input(shape=(config.max_len, ))\n",
    "    z = module(config, inputs)\n",
    "    outputs = tfk.layers.Dense(config.num_classes, activation='softmax')(z)\n",
    "    model = tfk.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253664f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "model = build_model(config, text_cnn)\n",
    "model.compile(\n",
    "    optimizer=tfk.optimizers.Adamax(learning_rate=1e-3),\n",
    "    loss=tfk.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tfk.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c109cc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 512, 256)     5408768     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 512, 256, 1)  0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 511, 1, 128)  65664       tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 510, 1, 128)  98432       tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 509, 1, 128)  131200      tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 128)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 1, 384)    0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze (TFOpLambd (None, 384)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 384)          0           tf.compat.v1.squeeze[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 20)           7700        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,711,764\n",
      "Trainable params: 5,711,764\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd602f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成训练数据\n",
    "max_len = 512\n",
    "batch_size = 32\n",
    "segment_name = \"20news-18828\"\n",
    "client = \"./data/\"  # 或者使用内置：GAS(token)\n",
    "# bert 主要用于 tokenize，目录下只有 `config.json` 和 `vocab.txt`\n",
    "data = NewsGroupSegment(client, segment_name, \"./bert/\", \"labels.txt\", max_len)\n",
    "epochs = 10\n",
    "steps_per_epoch = math.ceil(len(data.segment) / batch_size)\n",
    "\n",
    "dataset = Dataset.from_generator(\n",
    "    data,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(max_len, ), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "    ),\n",
    ").shuffle(buffer_size=len(data.segment), reshuffle_each_iteration=True).batch(batch_size).repeat(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847aab7",
   "metadata": {},
   "source": [
    "根目录如下：\n",
    "\n",
    "```bash\n",
    " tree . -L 2\n",
    ".\n",
    "├── bert\n",
    "│   ├── config.json\n",
    "│   └── vocab.txt\n",
    "├── data\n",
    "│   └── 20news-18828\n",
    "├── labels.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "36a9d50d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "589/589 [==============================] - 391s 564ms/step - loss: 2.3371 - sparse_categorical_accuracy: 0.3687\n",
      "Epoch 2/10\n",
      "589/589 [==============================] - 411s 592ms/step - loss: 1.2920 - sparse_categorical_accuracy: 0.6325\n",
      "Epoch 3/10\n",
      "589/589 [==============================] - 480s 709ms/step - loss: 0.9563 - sparse_categorical_accuracy: 0.7256\n",
      "Epoch 4/10\n",
      "589/589 [==============================] - 490s 737ms/step - loss: 0.7665 - sparse_categorical_accuracy: 0.7844\n",
      "Epoch 5/10\n",
      "589/589 [==============================] - 547s 836ms/step - loss: 0.6326 - sparse_categorical_accuracy: 0.8221\n",
      "Epoch 6/10\n",
      "589/589 [==============================] - 368s 522ms/step - loss: 0.5288 - sparse_categorical_accuracy: 0.8528\n",
      "Epoch 7/10\n",
      "589/589 [==============================] - 435s 625ms/step - loss: 0.4422 - sparse_categorical_accuracy: 0.8812\n",
      "Epoch 8/10\n",
      "589/589 [==============================] - 494s 706ms/step - loss: 0.3728 - sparse_categorical_accuracy: 0.9026\n",
      "Epoch 9/10\n",
      "589/589 [==============================] - 679s 1s/step - loss: 0.3131 - sparse_categorical_accuracy: 0.9152\n",
      "Epoch 10/10\n",
      "589/589 [==============================] - 676s 976ms/step - loss: 0.2641 - sparse_categorical_accuracy: 0.9348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169064ac0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81c668",
   "metadata": {},
   "source": [
    "使用中可能遇到的问题和注意事项：\n",
    "\n",
    "- 需要注意：数据集文件或 Segment 无法 shuffle，希望以后可以优化下，按文件随机选择某个文件中的样例\n",
    "- 数据加载比较慢，尤其是在线加载，可以先把文件 Load 到本地\n",
    "- 20Newsgroup 数据集类别有误，只有 16 个类别，实际 20 个"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412189d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "本文我们借助 TensorBay，使用 TextCNN 完成了简单的文本分类任务。TensorBay 本身并没有什么黑魔法，只是提供了一个统一管理数据的视角和工具，借助该工具，我们可以方便快捷地管理和使用数据集。\n",
    "\n",
    "本文使用 Tensorflow 为例进行说明，TensorBay 也支持 PyTorch，使用方法大同小异，具体可以参考文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113aa15d",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [TensorFlow — TensorBay documentation](https://tensorbay-python-sdk.graviti.com/en/stable/integrations/tensorflow.html)\n",
    "- [Home Page for 20 Newsgroups Data Set](http://qwone.com/~jason/20Newsgroups/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce6ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
