{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6a7dd6",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#数据集\" data-toc-modified-id=\"数据集-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>数据集 </a></span></li><li><span><a href=\"#文本预处理\" data-toc-modified-id=\"文本预处理-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>文本预处理 </a></span></li><li><span><a href=\"#Tokenize\" data-toc-modified-id=\"Tokenize-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Tokenize</a></span></li><li><span><a href=\"#数据管理\" data-toc-modified-id=\"数据管理-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>数据管理 </a></span></li><li><span><a href=\"#模型表征\" data-toc-modified-id=\"模型表征-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>模型表征 </a></span><ul class=\"toc-item\"><li><span><a href=\"#词典/规则\" data-toc-modified-id=\"词典/规则-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>词典 / 规则 </a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Naive Bayes</a></span></li><li><span><a href=\"#TextCNN\" data-toc-modified-id=\"TextCNN-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>TextCNN</a></span></li><li><span><a href=\"#Ernie\" data-toc-modified-id=\"Ernie-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Ernie</a></span></li><li><span><a href=\"#Skep\" data-toc-modified-id=\"Skep-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Skep</a></span></li></ul></li><li><span><a href=\"#Taskflow\" data-toc-modified-id=\"Taskflow-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Taskflow</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319d87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import pnlp\n",
    "from pnlp import Text, num_norm, cut_zhchar, MagicDict\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ahocorasick\n",
    "from Levenshtein import jaro\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from LAC import LAC\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.optimizer as optim\n",
    "from paddlenlp.data import Pad\n",
    "import paddlenlp\n",
    "from paddlenlp.transformers import SkepForSequenceClassification\n",
    "from paddlenlp import Taskflow\n",
    "\n",
    "ROOT = Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921aab24",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd360af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    \n",
    "    file_path: Path\n",
    "    test_size: float = 0.2\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.df = pd.read_csv(self.file_path, sep=\"\\t\")\n",
    "        self.train, self.test = self.split()\n",
    "        \n",
    "    def split(self):\n",
    "        return train_test_split(self.df, test_size=self.test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba0c9b",
   "metadata": {},
   "source": [
    "## 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72038f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PreProcessor:\n",
    "    \n",
    "    rules: List[str] = field(\n",
    "        default_factory=lambda: ['pic', 'lnk'])\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.clean_rule = Text(self.rules)\n",
    "    \n",
    "    def clean(self, text: str) -> str:\n",
    "        return self.clean_rule.clean(text)\n",
    "    \n",
    "    def normalize(self, text: str) -> str:\n",
    "        return text\n",
    "    \n",
    "    def __call__(self, text: str) -> str:\n",
    "        return self.normalize(self.clean(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea39e1",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46272393",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Tokenzier:\n",
    "    \n",
    "    type: str = \"word\"\n",
    "    vocab_path: Path = ROOT / \"vocab.txt\"\n",
    "    max_len: int = 128\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.word_segmentor = LAC(mode=\"seg\")\n",
    "        self.vocab = []\n",
    "        self.word2id = {}\n",
    "        if self.vocab_path.exists():\n",
    "            self.load_vocab(self.vocab_path)\n",
    "    \n",
    "    def tokenize2word(self, text: str) -> List[str]:\n",
    "        return self.word_segmentor.run(text)\n",
    "    \n",
    "    def tokenize2char(self, text: str) -> List[str]:\n",
    "        return cut_zhchar(text)\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return getattr(self, \"tokenize2\" + self.type)(text)\n",
    "    \n",
    "    def token2id(self, tokens: List[str]) -> List[int]:\n",
    "        res = []\n",
    "        for token in tokens:\n",
    "            id = self.word2id.get(token, 1)\n",
    "            res.append(id)\n",
    "        return res\n",
    "    \n",
    "    def load_vocab(self, path: Path):\n",
    "        self.vocab = pnlp.read_lines(path)\n",
    "        self.word2id = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "    \n",
    "    def build_vocab(self, sents: List[str]):\n",
    "        count = Counter()\n",
    "        for sent in sents:\n",
    "            words = self.tokenize(sent)\n",
    "            count.update(words)     \n",
    "        sort = sorted(count.items(), key=lambda x: x[1], reverse=True)\n",
    "        vocab = [w for w,f in sort if f >= 5]\n",
    "        self.vocab = [\"<PAD>\", \"<UNK>\"] + vocab\n",
    "        self.word2id = {word[0]: i for i, word in enumerate(self.vocab)}\n",
    "        pnlp.write_file(self.vocab_path, vocab)\n",
    "    \n",
    "    def __call__(self, texts: str) -> List[int]:\n",
    "        if type(texts) == str:\n",
    "            texts = [texts]\n",
    "        res = []\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            ids = self.token2id(tokens)\n",
    "            ids = ids[:self.max_len]\n",
    "            res.append(ids)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fac695",
   "metadata": {},
   "source": [
    "## 数据管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3e6e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataLoader:\n",
    "    \n",
    "    \n",
    "    file_path: Path\n",
    "    pretrain: str = \"\"\n",
    "    test_size: float = 0.2\n",
    "    rules = ['pic', 'lnk']\n",
    "    token_type: str = \"word\"\n",
    "    vocab_path: Path = ROOT / \"vocab.txt\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.ds = Dataset(self.file_path, self.test_size)\n",
    "        self.pp = PreProcessor(self.rules)\n",
    "        if self.pretrain:\n",
    "            self.tk = paddlenlp.transformers.ErnieTokenizer.from_pretrained(self.pretrain)\n",
    "        else:\n",
    "            self.tk = Tokenzier(self.token_type, self.vocab_path)\n",
    "            if not self.vocab_path.exists():\n",
    "                self.tk.build_vocab(self.ds.train[\"text_a\"])\n",
    "            else:\n",
    "                self.tk.load_vocab(self.vocab_path)\n",
    "    \n",
    "    def padding(self, ids: List[List[int]]):\n",
    "        return Pad(pad_val=0)(ids)\n",
    "    \n",
    "    def token_label(self, type: str = \"train\"):\n",
    "        data = getattr(self.ds , type)\n",
    "        for item in self.ds.train.itertuples(index=False):\n",
    "            tokens = self.tk.tokenize(self.pp(item.text_a))\n",
    "            yield tokens, item.label\n",
    "    \n",
    "    def ids_label(self, type: str = \"train\", batch_size: int = 64):\n",
    "        data = getattr(self.ds , type)\n",
    "        i = 0\n",
    "        batch, labels = [], []\n",
    "        for item in self.ds.train.itertuples(index=False):\n",
    "            if self.pretrain:\n",
    "                ids = self.tk(self.pp(item.text_a))[\"input_ids\"]\n",
    "            else:\n",
    "                ids = self.tk(self.pp(item.text_a))[0]\n",
    "            batch.append(ids)\n",
    "            labels.append(item.label)\n",
    "            i += 1\n",
    "            if i == batch_size:\n",
    "                yield self.padding(batch), np.array(labels)\n",
    "                batch, labels = [], []\n",
    "                i = 0\n",
    "        if batch:\n",
    "            yield self.padding(batch), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1a3ad6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-10-23 02:38:08,053] [    INFO]\u001b[0m - Already cached /Users/Yam/.paddlenlp/models/ernie-1.0/vocab.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ROOT / \"NLPCC14-SC/train.tsv\", pretrain=\"ernie-1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929fc3d1",
   "metadata": {},
   "source": [
    "## 模型表征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6af4bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Model:\n",
    "    \n",
    "    def evaluate(self, data: List[Tuple[List[str], int]]) -> float:\n",
    "        error = 0\n",
    "        i = 0\n",
    "        res = []\n",
    "        for tokens, label in data:\n",
    "            pred = self.predict(tokens)\n",
    "            error += (pred != label)\n",
    "            i += 1\n",
    "            res.append(pred)\n",
    "        return error / i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd093a56",
   "metadata": {},
   "source": [
    "### 词典/规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "672e579e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DictModel(Model):\n",
    "    \n",
    "    dict_path: Path = ROOT / \"dict\"\n",
    "    top_n: int = 100\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.pos = pnlp.read_pickle(self.dict_path / \"pos.pkl\")\n",
    "        self.neg = pnlp.read_pickle(self.dict_path / \"neg.pkl\")\n",
    "        self.pos_sample = np.random.choice(self.pos, size=self.top_n, replace=False).tolist()\n",
    "        self.neg_sample = np.random.choice(self.neg, size=self.top_n, replace=False).tolist()\n",
    "        self.model = self.build_aho(self.pos, self.neg)\n",
    "        self.model.make_automaton()\n",
    "    \n",
    "    def build_aho(self, pos: List[str], neg: List[str]):\n",
    "        aho = ahocorasick.Automaton()\n",
    "        for idx, key in enumerate(pos):\n",
    "            aho.add_word(key, (1, key))\n",
    "        for idx, key in enumerate(neg):\n",
    "            aho.add_word(key, (-1, key))\n",
    "        return aho\n",
    "    \n",
    "    def search(self, text: str) -> int:\n",
    "        i = 0\n",
    "        for end_index, (val, original_value) in self.model.iter(text):\n",
    "            i += val\n",
    "        return i\n",
    "    \n",
    "    def _match(self, sample: List[str], text: str) -> float:\n",
    "        res = 0.0\n",
    "        for v in sample:\n",
    "            res += jaro(text, v)\n",
    "        return res\n",
    "    \n",
    "    def predict(self, data: List[str]) -> int:\n",
    "        num = self.search(\" \".join(data))\n",
    "        if num == 0:\n",
    "            text = \"\".join(data)\n",
    "            return int(self._match(self.pos_sample, text) > self._match(self.neg_sample, text))\n",
    "        else:\n",
    "            return int(num > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "60cc17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DictModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "b3d8c188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50025"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.evaluate(dl.token_label(\"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56496415",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58e7955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NaiveBayes(Model):\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.pos_prob = {}\n",
    "        self.pos_prior = 0.5\n",
    "        self.neg_prob = {}\n",
    "        self.neg_prior = 0.5\n",
    "    \n",
    "    def _train(self, data: List[str]) -> dict:\n",
    "        res = []\n",
    "        count = Counter(data)\n",
    "        prob = {}\n",
    "        length = len(data)\n",
    "        for k,v in count.items():\n",
    "            prob[k] = v / length\n",
    "        return prob, length\n",
    "        \n",
    "\n",
    "    def train(self, data: List[str]):\n",
    "        pos, neg, labels = [], [], []\n",
    "        for tokens, label in data:\n",
    "            labels.append(label)\n",
    "            if label == 1:\n",
    "                pos.extend(tokens)\n",
    "            else:\n",
    "                neg.extend(tokens)\n",
    "        self.pos_prob, len_pos = self._train(pos)\n",
    "        self.neg_prob, len_neg = self._train(neg)\n",
    "        \n",
    "        length = len_pos + len_neg\n",
    "        label_count = Counter(labels)\n",
    "        self.pos_prior = label_count[1] / length\n",
    "        self.neg_prior = label_count[0] / length\n",
    "    \n",
    "    def predict(self, data: List[str]) -> int:\n",
    "        res = np.log(self.pos_prior / self.neg_prior)\n",
    "        for w in data:\n",
    "            res += np.log(self.pos_prob.get(w, 1) / self.neg_prob.get(w, 1))\n",
    "        return res > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8361a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6cae23d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.train(dl.token_label(\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "092e4e34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.718"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.evaluate(dl.token_label(\"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc074230",
   "metadata": {},
   "source": [
    "### TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7048109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(paddle.nn.Layer):\n",
    "    def __init__(self, config):\n",
    "        super(TextCNN, self).__init__()\n",
    "        if not config.pretrained:\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=config.vocab_size, \n",
    "                embedding_dim=config.embed_size, \n",
    "                padding_idx=0,\n",
    "                weight_attr=config.pretrained)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=config.vocab_size, \n",
    "                embedding_dim=config.embed_size, \n",
    "                padding_idx=0)\n",
    "        self.convs = nn.LayerList(\n",
    "            [nn.Conv2D(1, config.num_filters, (kernel_size_, config.embed_size)) \n",
    "             for kernel_size_ in config.filter_sizes])\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.linear = nn.Linear(3 * config.num_filters, config.num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x).unsqueeze(1)\n",
    "        convs = [nn.ReLU()(conv(embedding)).squeeze(3) for conv in self.convs]\n",
    "        pool_out = [nn.MaxPool1D(block.shape[2])(block).squeeze(2) for block in convs]\n",
    "        pool_out = paddle.concat(pool_out, 1)\n",
    "        logits = self.linear(pool_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f9dd5d4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(model, dl):\n",
    "    optimizer = optim.Adam(parameters=model.parameters(), learning_rate=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        train_loss, test_loss = [], []\n",
    "        train_acc, test_acc = [], []\n",
    "        model.train()\n",
    "        for i, (x, y) in enumerate(dl.ids_label(\"train\", BATCH_SIZE)):\n",
    "            x = paddle.Tensor(x)\n",
    "            y = paddle.Tensor(y).unsqueeze(1)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append(paddle.metric.accuracy(pred, y).numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()\n",
    "        model.eval()\n",
    "        for i, (x, y) in enumerate(dl.ids_label(\"test\", BATCH_SIZE)):\n",
    "            x = paddle.Tensor(x)\n",
    "            y = paddle.Tensor(y).unsqueeze(1)\n",
    "            pred = model(x)\n",
    "            test_loss.append(criterion(pred, y).item())\n",
    "            test_acc.append(paddle.metric.accuracy(pred, y).numpy())\n",
    "        print(\n",
    "            \"Epoch: [{}/{}] TrainLoss/TestLoss: {:.4f}/{:.4f} TrainAcc/TestAcc: {:.4f}/{:.4f}\".format(\n",
    "            epoch + 1, EPOCHS,\n",
    "            np.mean(train_loss), np.mean(test_loss),\n",
    "            np.mean(train_acc), np.mean(test_acc))\n",
    "        )\n",
    "    paddle.save(cnn.state_dict(), \"save/\" + model.full_name() +\".pdparams\")\n",
    "    paddle.save(optimizer.state_dict(), \"save/\" + model.full_name() + \"_Adam.pdparams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ce42405",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MagicDict({\n",
    "    \"vocab_size\": len(dl.tk.vocab),\n",
    "    \"embed_size\": 128,\n",
    "    \"dropout\": 0.5,\n",
    "    \"filter_sizes\": [2,3,4],\n",
    "    \"num_filters\": 128,\n",
    "    \"num_labels\": 2,\n",
    "    \"pretrained\": None\n",
    "    \n",
    "})\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "cnn = TextCNN(config)\n",
    "dl = DataLoader(ROOT / \"NLPCC14-SC/train.tsv\", pretrain=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e7a986b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5] TrainLoss/TestLoss: 0.5883/0.4185 TrainAcc/TestAcc: 0.6973/0.8269\n",
      "Epoch: [2/5] TrainLoss/TestLoss: 0.4122/0.2714 TrainAcc/TestAcc: 0.8266/0.9054\n",
      "Epoch: [3/5] TrainLoss/TestLoss: 0.2845/0.1615 TrainAcc/TestAcc: 0.8956/0.9541\n",
      "Epoch: [4/5] TrainLoss/TestLoss: 0.1774/0.0900 TrainAcc/TestAcc: 0.9411/0.9796\n",
      "Epoch: [5/5] TrainLoss/TestLoss: 0.1041/0.0511 TrainAcc/TestAcc: 0.9691/0.9885\n"
     ]
    }
   ],
   "source": [
    "train(cnn, dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a9fa9",
   "metadata": {},
   "source": [
    "### Ernie\n",
    "\n",
    "预训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b196c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ernie(paddle.nn.Layer):\n",
    "    def __init__(self, config):\n",
    "        super(Ernie, self).__init__()\n",
    "        self.ernie_model = paddlenlp.transformers.ErnieModel.from_pretrained(config.pretrained)\n",
    "        self.linear = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        sequence_output, pooled_output = self.ernie_model(x)\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e94153da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-10-23 02:38:31,265] [    INFO]\u001b[0m - Already cached /Users/Yam/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\u001b[0m\n",
      "\u001b[32m[2021-10-23 02:38:39,071] [    INFO]\u001b[0m - Weights from pretrained model not used in ErnieModel: ['cls.predictions.layer_norm.weight', 'cls.predictions.decoder_bias', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.predictions.layer_norm.bias']\u001b[0m\n",
      "\u001b[32m[2021-10-23 02:38:39,418] [    INFO]\u001b[0m - Already cached /Users/Yam/.paddlenlp/models/ernie-1.0/vocab.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = MagicDict({\n",
    "    \"pretrained\": \"ernie-1.0\",\n",
    "    \"hidden_size\": 768,\n",
    "    \"num_labels\": 2,\n",
    "})\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "ernie = Ernie(config)\n",
    "dl = DataLoader(ROOT / \"NLPCC14-SC/train.tsv\", pretrain=\"ernie-1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28c4d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in dl.ids_label(\"train\", 2): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee93926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3bb7ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 2], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
       "       [[-1.29681790,  2.99378061],\n",
       "        [-1.45288467,  0.72905755]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ernie(paddle.Tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c7fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小心你的 CPU\n",
    "train(ernie, dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1c5ff0",
   "metadata": {},
   "source": [
    "### Skep\n",
    "\n",
    "直接输出 logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9080383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skep(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Skep, self).__init__()\n",
    "        self.skep_model = SkepForSequenceClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.skep_model(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18cb5f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-10-23 02:46:01,290] [    INFO]\u001b[0m - Already cached /Users/Yam/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\u001b[0m\n",
      "\u001b[32m[2021-10-23 02:46:43,144] [    INFO]\u001b[0m - Already cached /Users/Yam/.paddlenlp/models/ernie-1.0/vocab.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "skep = Skep()\n",
    "dl = DataLoader(ROOT / \"NLPCC14-SC/train.tsv\", pretrain=\"ernie-1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c72a8dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 2], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
       "       [[-0.11525318, -0.39649525],\n",
       "        [ 0.05798459, -0.43128473]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skep(paddle.Tensor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c47903",
   "metadata": {},
   "source": [
    "## Taskflow\n",
    "\n",
    "直接输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ef2ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-10-23 02:59:48,427] [    INFO]\u001b[0m - Converting to the inference model cost a little time.\u001b[0m\n",
      "\u001b[32m[2021-10-23 02:59:54,543] [    INFO]\u001b[0m - The inference model save in the path:/Users/Yam/.paddlenlp/taskflow/sentiment_analysis/bilstm/static/inference\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "senta = Taskflow(\"sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38fd52d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片',\n",
       "  'label': 'negative',\n",
       "  'score': 0.6691399216651917}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senta(\"怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f80429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
