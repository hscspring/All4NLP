{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "319d87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import pnlp\n",
    "from pnlp import Text, num_norm, cut_zhchar\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ahocorasick\n",
    "from Levenshtein import jaro\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from LAC import LAC\n",
    "\n",
    "ROOT = Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921aab24",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3dd360af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    \n",
    "    file_path: Path\n",
    "    test_size: float = 0.2\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.df = pd.read_csv(self.file_path, sep=\"\\t\")\n",
    "        self.train, self.test = self.split()\n",
    "        \n",
    "    def split(self):\n",
    "        return train_test_split(self.df, test_size=self.test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba0c9b",
   "metadata": {},
   "source": [
    "## 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e72038f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PreProcessor:\n",
    "    \n",
    "    rules: List[str] = field(\n",
    "        default_factory=lambda: ['pic', 'lnk'])\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.clean_rule = Text(self.rules)\n",
    "    \n",
    "    def clean(self, text: str) -> str:\n",
    "        return self.clean_rule.clean(text)\n",
    "    \n",
    "    def normalize(self, text: str) -> str:\n",
    "        return text\n",
    "    \n",
    "    def __call__(self, text: str) -> str:\n",
    "        return self.normalize(self.clean(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea39e1",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "46272393",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Tokenzier:\n",
    "    \n",
    "    dict_path: str = None\n",
    "    type: str = \"word\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.word_segmentor = LAC(mode=\"seg\")\n",
    "    \n",
    "    def tokenize2word(self, text: str) -> List[str]:\n",
    "        return self.word_segmentor.run(text)\n",
    "    \n",
    "    def tokenize2char(self, text: str) -> List[str]:\n",
    "        return cut_zhchar(text)\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return getattr(self, \"tokenize2\" + self.type)(text)\n",
    "    \n",
    "    def __call__(self, text: str) -> List[int]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fac695",
   "metadata": {},
   "source": [
    "## 特征表征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b3e6e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataLoader:\n",
    "    \n",
    "    file_path: Path\n",
    "    test_size: float = 0.2\n",
    "    rules = ['pic', 'lnk']\n",
    "    dict_path = None\n",
    "    token_type: str = \"word\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.ds = Dataset(self.file_path, self.test_size)\n",
    "        self.pp = PreProcessor(self.rules)\n",
    "        self.tk = Tokenzier(self.dict_path, self.token_type)\n",
    "    \n",
    "    def dm_data(self, type: str = \"train\"):\n",
    "        data = getattr(self.ds , type)\n",
    "        for item in self.ds.train.itertuples(index=False):\n",
    "            tokens = self.tk.tokenize(self.pp(item.text_a))\n",
    "            yield tokens, item.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "838193aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ROOT / \"NLPCC14-SC/train.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd093a56",
   "metadata": {},
   "source": [
    "### 词典/规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "672e579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DictModel:\n",
    "    \n",
    "    top_n: int = 3000\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.pos = []\n",
    "        self.neg = []\n",
    "    \n",
    "    def build_aho(self, pos: List[str], neg: List[str]):\n",
    "        aho = ahocorasick.Automaton()\n",
    "        for idx, key in enumerate(pos):\n",
    "            aho.add_word(key, (1, key))\n",
    "        for idx, key in enumerate(neg):\n",
    "            aho.add_word(key, (-1, key))\n",
    "        return aho\n",
    "    \n",
    "    def search(self, text: str) -> int:\n",
    "        i = 0\n",
    "        for end_index, (val, original_value) in self.aho.iter(text):\n",
    "            i += val\n",
    "        return i\n",
    "    \n",
    "    def _match(self, dct: List[str], text: str) -> float:\n",
    "        res = 0.0\n",
    "        sample = np.random.choice(dct, size=100, replace=False)\n",
    "        for v in sample:\n",
    "            res += jaro(text, v)\n",
    "        return res / len(dct)\n",
    "    \n",
    "    def match(self, text: str) -> int:\n",
    "        return int(self._match(self.pos, text) > self._match(self.neg, text))\n",
    "    \n",
    "    def train(self, data: List[Tuple[List[str], int]]):\n",
    "        for tokens,label in data:\n",
    "            if label == 1:\n",
    "                self.pos.extend(tokens)\n",
    "            else:\n",
    "                self.neg.extend(tokens)\n",
    "        pos_count = [v for v,f in Counter(self.pos).most_common(self.top_n)]\n",
    "        neg_count = [v for v,f in Counter(self.neg).most_common(self.top_n)]\n",
    "        self.aho = self.build_aho(pos_count, neg_count)\n",
    "        self.aho.make_automaton()\n",
    "        \n",
    "    \n",
    "    def predict(self, data: List[str]) -> int:\n",
    "        num = self.search(\" \".join(data))\n",
    "        return self.match(\"\".join(data)) if num == 0 else int(num > 0)\n",
    "    \n",
    "    def evaluate(self, data: List[Tuple[List[str], int]]) -> float:\n",
    "        error = 0\n",
    "        i = 0\n",
    "        for tokens, label in data:\n",
    "            pred = self.predict(tokens)\n",
    "            error += (pred != label)\n",
    "            i += 1\n",
    "        return error / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "60cc17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DictModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "9d7e25aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.train(dl.dm_data(\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4875e01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4985"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.evaluate(dl.dm_data(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ac5d6908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['百度', '是', '一家', '科技', '公司', '。']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lac.run(\"百度是一家科技公司。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6a34c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as tfk\n",
    "import tensorflow as tf\n",
    "from hnlp import gen_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296cfec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30975/30975 [00:09<00:00, 3214.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp import Taskflow\n",
    "seg = Taskflow(\"word_segmentation\")\n",
    "seg(\"百度是一家科技公司。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06899490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['百度', '是', '一家', '科技', '公司', '。']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg(\"百度是一家科技公司。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15843750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b274b985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e9269125",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = tfk.layers.Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=(2, 300),\n",
    "            strides=(1, 1),\n",
    "            padding=\"valid\",\n",
    "            data_format=\"channels_last\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"glorot_normal\",\n",
    "            bias_initializer=tfk.initializers.constant(0.1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "00255639",
   "metadata": {},
   "outputs": [],
   "source": [
    "emd = gen_hidden(1, 30, 300, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "5f871327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 29, 1, 1), dtype=float32, numpy=\n",
       "array([[[[0.        ]],\n",
       "\n",
       "        [[0.00795222]],\n",
       "\n",
       "        [[0.17629689]],\n",
       "\n",
       "        [[0.1786648 ]],\n",
       "\n",
       "        [[0.09023976]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.07314899]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.0391703 ]],\n",
       "\n",
       "        [[0.20579764]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.21271077]],\n",
       "\n",
       "        [[0.13188985]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.3476765 ]],\n",
       "\n",
       "        [[0.2666411 ]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.500645  ]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.        ]],\n",
       "\n",
       "        [[0.23655248]]]], dtype=float32)>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cf3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37dfd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f80429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
