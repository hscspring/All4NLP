{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Blog: [Luong Attention Understanding | Yam](https://yam.gift/2020/04/14/Paper/2020-04-14-Luong-Attention/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/Documents/Study/DL-Models/pytorch-batch-luong-attention/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embeddings import create_embedding_maps\n",
    "from utils.batches import batches, data_from_batch\n",
    "from models.luong_attention import luong_attention\n",
    "from torch.autograd import Variable\n",
    "from utils.tokens import Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "def load_data(dataset_module, train_dir, debug_restrict_data):\n",
    "    dataset_module_path = f\"utils.load_and_preprocessing.{dataset_module}\"\n",
    "    dataset_module = importlib.import_module(dataset_module_path)\n",
    "    train, val = dataset_module.load_data(train_dir, debug_restrict_data)\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10835/10835 [00:00<00:00, 970679.74it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 48044.72it/s]\n"
     ]
    }
   ],
   "source": [
    "train, val = load_data(\"translation\", \"data/translation\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10835, 2)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>je vais dormir .</td>\n",
       "      <td>i am going to bed .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>je suis presque prete .</td>\n",
       "      <td>i am almost ready .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tu es encore un bleu .</td>\n",
       "      <td>you re still green .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c est toi qui m as entraine .</td>\n",
       "      <td>you re the one who trained me .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on apprend encore a se connaitre .</td>\n",
       "      <td>we re still getting to know each other .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               source  \\\n",
       "0                    je vais dormir .   \n",
       "1             je suis presque prete .   \n",
       "2              tu es encore un bleu .   \n",
       "3       c est toi qui m as entraine .   \n",
       "4  on apprend encore a se connaitre .   \n",
       "\n",
       "                                     target  \n",
       "0                       i am going to bed .  \n",
       "1                       i am almost ready .  \n",
       "2                      you re still green .  \n",
       "3           you re the one who trained me .  \n",
       "4  we re still getting to know each other .  "
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 10835/10835 [00:00<00:00, 111409.53it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  9.12it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 45003.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 15.54it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10835 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 10835/10835 [00:00<00:00, 86121.99it/s][A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  7.59it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 36503.95it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 13.93it/s]\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_map, \\\n",
    "decoder_embedding_map, \\\n",
    "encoder_embedding_matrix, \\\n",
    "decoder_embedding_matrix = create_embedding_maps(train, val, 64, different_vocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4490"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_embedding_map.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4490, 64])"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_embedding_matrix.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2927, 64])"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_embedding_matrix.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = luong_attention.LuongAttnDecoderRNN(\"general\", 32, 64, \n",
    "                                              decoder_embedding_map.n_words, \n",
    "                                              2, 0.1, decoder_embedding_matrix, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LuongAttnDecoderRNN(\n",
       "  (embedding): Embedding(2927, 64)\n",
       "  (gru): GRU(64, 32, num_layers=2, dropout=0.1)\n",
       "  (concat): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (out): Linear(in_features=32, out_features=2927, bias=True)\n",
       "  (attn): Attn(\n",
       "    (attn): Linear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = luong_attention.EncoderRNN(32, 64, 2, 0.1, encoder_embedding_matrix, \"GRU\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embedding): Embedding(4490, 64)\n",
       "  (rnn): GRU(64, 32, num_layers=2, dropout=0.1, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches(train, encoder_embedding_map, decoder_embedding_map, use_cuda=False, batch_size=4):\n",
    "    batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_var': tensor([[  11,   48,    4,   32],\n",
       "         [  49, 2573,   54,   17],\n",
       "         [1186,  637, 3453, 1159],\n",
       "         [  52,  747,    7,    7],\n",
       "         [ 164,    7,    3,    3],\n",
       "         [1187,    3,    0,    0],\n",
       "         [ 128,    0,    0,    0],\n",
       "         [1188,    0,    0,    0],\n",
       "         [   7,    0,    0,    0],\n",
       "         [   3,    0,    0,    0]]),\n",
       " 'source_lengths': [10, 6, 5, 5],\n",
       " 'target_var': tensor([[  12,   21,    4,   31],\n",
       "         [  13,   13,   46,   54],\n",
       "         [  44, 1844, 1961, 1938],\n",
       "         [  29, 1845,    9,    9],\n",
       "         [ 931,    9,    3,    3],\n",
       "         [   9,    3,    0,    0],\n",
       "         [   3,    0,    0,    0]]),\n",
       " 'target_lengths': [7, 6, 5, 5]}"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_var, source_lengths, target_var, target_lengths = data_from_batch(batch)\n",
    "current_batch_size = len(target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_hidden = encoder(source_var, encoder.init_hidden(current_batch_size), source_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 32])"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 32])"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Variable(torch.LongTensor([Tokens.SOS_token] * current_batch_size))\n",
    "decoder_hidden = encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_target_length = max(target_lengths)\n",
    "all_decoder_outputs = Variable(torch.zeros(max_target_length, current_batch_size, decoder.output_size))\n",
    "max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 2927])"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 32])"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 32])"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(max_target_length):\n",
    "    decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    all_decoder_outputs[t] = decoder_output\n",
    "    decoder_input = target_var[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 2927])"
      ]
     },
     "execution_count": 918,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2927])"
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 32])"
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 10])"
      ]
     },
     "execution_count": 921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.masked_cross_entropy import masked_cross_entropy, sequence_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HaoShaochun/Documents/Study/DL-Models/pytorch-batch-luong-attention/utils/masked_cross_entropy.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs_flat = F.log_softmax(logits_flat)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(7.9472, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_cross_entropy(all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "                                        target_var.transpose(0, 1).contiguous(),\n",
    "                                        target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target_var.transpose(0, 1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 2927])"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = all_decoder_outputs.transpose(0, 1).contiguous()\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 2927])"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "logits_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_flat = F.log_softmax(logits_flat, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 1])"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_flat = target.view(-1, 1)\n",
    "target_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_flat: (batch * max_len, 1)\n",
    "losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7])"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = losses_flat.view(*target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7])"
      ]
     },
     "execution_count": 888,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 6, 5, 5]) 7\n"
     ]
    }
   ],
   "source": [
    "print(Variable(torch.LongTensor(target_lengths)), target.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sequence_mask(sequence_length=Variable(torch.LongTensor(target_lengths)), max_len=target.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sequence_length=Variable(torch.LongTensor(target_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 6, 5, 5])"
      ]
     },
     "execution_count": 907,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 910,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = sequence_length.size(0)\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_range = torch.arange(0, max_len).long()\n",
    "seq_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6],\n",
       "        [0, 1, 2, 3, 4, 5, 6],\n",
       "        [0, 1, 2, 3, 4, 5, 6],\n",
       "        [0, 1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "seq_range_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6],\n",
       "        [0, 1, 2, 3, 4, 5, 6],\n",
       "        [0, 1, 2, 3, 4, 5, 6],\n",
       "        [0, 1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_range_expand = Variable(seq_range_expand)\n",
    "seq_range_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 7, 7, 7, 7, 7, 7],\n",
       "        [6, 6, 6, 6, 6, 6, 6],\n",
       "        [5, 5, 5, 5, 5, 5, 5],\n",
       "        [5, 5, 5, 5, 5, 5, 5]])"
      ]
     },
     "execution_count": 914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length_expand = sequence_length.unsqueeze(1).expand_as(seq_range_expand)\n",
    "seq_length_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False]])"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = seq_range_expand < seq_length_expand\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.8367, 7.9381, 7.8862, 7.8695, 8.0942, 7.8369, 8.1076],\n",
       "        [8.0719, 7.9407, 7.8208, 7.8800, 7.8822, 8.0856, 8.3650],\n",
       "        [7.9334, 7.9262, 7.9541, 7.7630, 8.1345, 8.3291, 8.3044],\n",
       "        [7.7619, 7.9629, 8.0988, 7.8717, 8.1288, 8.3198, 8.2923]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  12,   21,    4,   31],\n",
       "        [  13,   13,   46,   54],\n",
       "        [  44, 1844, 1961, 1938],\n",
       "        [  29, 1845,    9,    9],\n",
       "        [ 931,    9,    3,    3],\n",
       "        [   9,    3,    0,    0],\n",
       "        [   3,    0,    0,    0]])"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2927])"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "topv, topi = decoder_output.data.topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5208],\n",
       "        [0.4515],\n",
       "        [0.4341],\n",
       "        [0.5078]])"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 260],\n",
       "        [2120],\n",
       "        [2853],\n",
       "        [2853]])"
      ]
     },
     "execution_count": 938,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 936,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topi.squeeze().detach().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 952,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topi[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent', 'ideal', 'startled', 'startled']"
      ]
     },
     "execution_count": 961,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index2word(topi[i].item()) for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index2word(index: int):\n",
    "    return decoder_embedding_map.index2word[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=9):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        print(\"self embedding shape: \", self.embedding(input).shape)\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        print(\"embedded shape: \", embedded.shape)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        print(\"attn_weights shape: \", attn_weights.shape)\n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        print(\"attn_weights.unsqueeze(0)\", attn_weights.unsqueeze(0).shape)\n",
    "        print(\"attn_applied\", attn_applied.shape)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        print(\"self.attn_combine(output)\", self.attn_combine(output).shape)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        print(\"hidden shape\", hidden.shape)\n",
    "        hidden = hidden[0].unsqueeze(0)\n",
    "        print(\"hidden shape\", hidden.shape)\n",
    "        print(\"output shape\", output.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        print(\"output, hidden\", output.shape, hidden.shape)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        print(\"output\", output.shape)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_decoder = AttnDecoderRNN(32, decoder_embedding_map.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Variable(torch.LongTensor([Tokens.SOS_token] * current_batch_size))\n",
    "decoder_hidden = encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = nn.Embedding(2927, 32)(decoder_input).view(1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 32])"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output2, decoder_hidden2, attn_weights2 = attn_decoder(decoder_input, \n",
    "                                                               decoder_hidden, \n",
    "                                                               encoder_outputs.squeeze(1)\n",
    "                                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LuongAttn Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size, output_size,  n_layers, dropout):\n",
    "        super(LuongAttnDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.input_size)\n",
    "        self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.input_size, self.hidden_size, n_layers, dropout=self.dropout)\n",
    "        self.concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        batch_size = input.size(0)\n",
    "        print(input.shape)\n",
    "        print(self.embedding(input).shape)\n",
    "        embedded = self.embedding(input).view(1, batch_size, self.input_size)\n",
    "        print(embedded.shape)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        print(output.shape, hidden.shape)\n",
    "        attn_weights = F.softmax(torch.bmm(self.attn(output).transpose(1, 0), \n",
    "                                           encoder_outputs.permute(1, 2, 0)), 2)\n",
    "        print(attn_weights.shape)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        print(context.shape)\n",
    "        output = output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        print(output.shape, context.shape)\n",
    "        concat_input = torch.cat((output, context), 1)\n",
    "        print(concat_input.shape)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "        print(concat_output.shape)\n",
    "        output = self.out(concat_output)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "latt_decoder = LuongAttnDecoder(32, 64, 2927, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Variable(torch.LongTensor([Tokens.SOS_token] * current_batch_size))\n",
    "decoder_hidden = encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 32])"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([1, 4, 64])\n",
      "torch.Size([1, 4, 32]) torch.Size([2, 4, 32])\n",
      "torch.Size([4, 1, 8])\n",
      "torch.Size([4, 1, 32])\n",
      "torch.Size([4, 32]) torch.Size([4, 32])\n",
      "torch.Size([4, 64])\n",
      "torch.Size([4, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "decoder_output3, decoder_hidden3, attn_weights3 = latt_decoder(\n",
    "    decoder_input, decoder_hidden, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2927])"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 32])"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 8])"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 32])"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 8])"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.permute(1, 2, 0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20)\n",
    "input = torch.randn(1, 3, 10)\n",
    "h0 = torch.randn(1, 3, 20)\n",
    "c0 = torch.rand(1, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(10, 20)\n",
    "input = torch.randn(1, 3, 10)\n",
    "h0 = torch.randn(1, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 20])"
      ]
     },
     "execution_count": 965,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 20])"
      ]
     },
     "execution_count": 966,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(2927, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4,  32],\n",
       "        [180, 685],\n",
       "        [ 38,  93],\n",
       "        [ 36, 183],\n",
       "        [490, 200],\n",
       "        [ 59,   7],\n",
       "        [491,   3],\n",
       "        [  7,   0],\n",
       "        [  3,   0]])"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = embedding(source_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 2, 32])"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 288])"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.view(1, 2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 2])"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 7]"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(64, 32, 2, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 32])"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(8, 1, 10)\n",
    "mat2 = torch.randn(8, 10, 32)\n",
    "res = torch.bmm(input, mat2)\n",
    "res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
