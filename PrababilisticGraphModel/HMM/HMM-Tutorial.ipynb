{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#大纲\" data-toc-modified-id=\"大纲-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>大纲</a></span></li><li><span><a href=\"#基本概念\" data-toc-modified-id=\"基本概念-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>基本概念</a></span><ul class=\"toc-item\"><li><span><a href=\"#概率模型（Probabilistic-Models）\" data-toc-modified-id=\"概率模型（Probabilistic-Models）-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>概率模型（Probabilistic Models）</a></span></li><li><span><a href=\"#概率图模型（Probabilistic-Graphical-Models）\" data-toc-modified-id=\"概率图模型（Probabilistic-Graphical-Models）-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>概率图模型（Probabilistic Graphical Models）</a></span></li><li><span><a href=\"#贝叶斯网（Bayesian-Networks）\" data-toc-modified-id=\"贝叶斯网（Bayesian-Networks）-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>贝叶斯网（Bayesian Networks）</a></span></li><li><span><a href=\"#马尔科夫网（Markov-Networks）\" data-toc-modified-id=\"马尔科夫网（Markov-Networks）-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>马尔科夫网（Markov Networks）</a></span></li><li><span><a href=\"#马尔科夫网-VS-贝叶斯网\" data-toc-modified-id=\"马尔科夫网-VS-贝叶斯网-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>马尔科夫网 VS 贝叶斯网</a></span></li><li><span><a href=\"#马尔科夫模型（Markov-Model）\" data-toc-modified-id=\"马尔科夫模型（Markov-Model）-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>马尔科夫模型（Markov Model）</a></span></li><li><span><a href=\"#常见的图模型（Graphical-Model）\" data-toc-modified-id=\"常见的图模型（Graphical-Model）-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>常见的图模型（Graphical Model）</a></span></li><li><span><a href=\"#自然语言处理（NLP）中概率图模型的演变\" data-toc-modified-id=\"自然语言处理（NLP）中概率图模型的演变-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>自然语言处理（NLP）中概率图模型的演变</a></span></li><li><span><a href=\"#判别式（Discriminative-Model）-VS-生成式（Generative-Model）\" data-toc-modified-id=\"判别式（Discriminative-Model）-VS-生成式（Generative-Model）-2.9\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>判别式（Discriminative Model） VS 生成式（Generative Model）</a></span></li></ul></li><li><span><a href=\"#What\" data-toc-modified-id=\"What-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>What</a></span><ul class=\"toc-item\"><li><span><a href=\"#定义\" data-toc-modified-id=\"定义-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>定义</a></span></li><li><span><a href=\"#假设\" data-toc-modified-id=\"假设-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>假设</a></span></li><li><span><a href=\"#参数\" data-toc-modified-id=\"参数-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>参数</a></span></li></ul></li><li><span><a href=\"#How\" data-toc-modified-id=\"How-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>How</a></span><ul class=\"toc-item\"><li><span><a href=\"#过程\" data-toc-modified-id=\"过程-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>过程</a></span></li><li><span><a href=\"#基本问题\" data-toc-modified-id=\"基本问题-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>基本问题</a></span></li></ul></li><li><span><a href=\"#Why\" data-toc-modified-id=\"Why-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Why</a></span></li><li><span><a href=\"#实例\" data-toc-modified-id=\"实例-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>实例</a></span><ul class=\"toc-item\"><li><span><a href=\"#参数和状态\" data-toc-modified-id=\"参数和状态-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>参数和状态</a></span></li><li><span><a href=\"#随机生成观测和状态序列\" data-toc-modified-id=\"随机生成观测和状态序列-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>随机生成观测和状态序列</a></span></li><li><span><a href=\"#估计问题\" data-toc-modified-id=\"估计问题-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>估计问题</a></span><ul class=\"toc-item\"><li><span><a href=\"#直接计算\" data-toc-modified-id=\"直接计算-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;</span>直接计算</a></span></li><li><span><a href=\"#前向算法\" data-toc-modified-id=\"前向算法-6.3.2\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;</span>前向算法</a></span></li><li><span><a href=\"#后向算法\" data-toc-modified-id=\"后向算法-6.3.3\"><span class=\"toc-item-num\">6.3.3&nbsp;&nbsp;</span>后向算法</a></span></li></ul></li><li><span><a href=\"#学习问题\" data-toc-modified-id=\"学习问题-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>学习问题</a></span><ul class=\"toc-item\"><li><span><a href=\"#监督学习\" data-toc-modified-id=\"监督学习-6.4.1\"><span class=\"toc-item-num\">6.4.1&nbsp;&nbsp;</span>监督学习</a></span></li><li><span><a href=\"#Baum-Welch-算法\" data-toc-modified-id=\"Baum-Welch-算法-6.4.2\"><span class=\"toc-item-num\">6.4.2&nbsp;&nbsp;</span>Baum-Welch 算法</a></span></li></ul></li><li><span><a href=\"#预测问题\" data-toc-modified-id=\"预测问题-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>预测问题</a></span><ul class=\"toc-item\"><li><span><a href=\"#近似算法\" data-toc-modified-id=\"近似算法-6.5.1\"><span class=\"toc-item-num\">6.5.1&nbsp;&nbsp;</span>近似算法</a></span></li><li><span><a href=\"#维特比算法\" data-toc-modified-id=\"维特比算法-6.5.2\"><span class=\"toc-item-num\">6.5.2&nbsp;&nbsp;</span>维特比算法</a></span></li></ul></li><li><span><a href=\"#结果\" data-toc-modified-id=\"结果-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>结果</a></span></li></ul></li><li><span><a href=\"#题外\" data-toc-modified-id=\"题外-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>题外</a></span><ul class=\"toc-item\"><li><span><a href=\"#概率编程\" data-toc-modified-id=\"概率编程-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>概率编程</a></span></li><li><span><a href=\"#贝叶斯学派-VS-频率学派\" data-toc-modified-id=\"贝叶斯学派-VS-频率学派-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>贝叶斯学派 VS 频率学派</a></span></li></ul></li><li><span><a href=\"#参考\" data-toc-modified-id=\"参考-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>参考</a></span><ul class=\"toc-item\"><li><span><a href=\"#贝叶斯网-VS-马尔科夫网、马尔科夫模型\" data-toc-modified-id=\"贝叶斯网-VS-马尔科夫网、马尔科夫模型-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>贝叶斯网 VS 马尔科夫网、马尔科夫模型</a></span></li><li><span><a href=\"#HMM\" data-toc-modified-id=\"HMM-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>HMM</a></span></li><li><span><a href=\"#其他\" data-toc-modified-id=\"其他-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>其他</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大纲\n",
    "\n",
    "- 引入\n",
    "  - 概率模型\n",
    "  - 概率图模型\n",
    "  - 贝叶斯网\n",
    "  - 马尔科夫网\n",
    "- What\n",
    "  - 定义\n",
    "  - 假设\n",
    "  - 参数\n",
    "- How\n",
    "  - 过程\n",
    "  - 基本问题\n",
    "- Why\n",
    "  - 略\n",
    "- 题外\n",
    "  - 概率编程\n",
    "  - 贝叶斯学派 VS 频率学派"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本概念\n",
    "\n",
    "\n",
    "## 概率模型（Probabilistic Models）\n",
    "\n",
    "\n",
    "是编码对某个不确定情况一般知识的一种方式。  \n",
    "\n",
    "一般知识说明两个事实：\n",
    "\n",
    "- 可能的情况\n",
    "- 可能性较大的情况\n",
    "\n",
    "一般知识（所有设想的可能性和可能性中的趋势），表现为可能世界上的概率分布。  \n",
    "\n",
    "可能性决定可能世界的定义，趋势决定每个可能世界的概率。  \n",
    "\n",
    "概率模型就是估计不确定情况的概率分布。\n",
    "\n",
    "\n",
    "## 概率图模型（Probabilistic Graphical Models）\n",
    "\n",
    "\n",
    "在概率模型的基础上，使用了基于图的方法来表示概率分布。  \n",
    "一种通用化的不确定性知识表示和处理方法。  \n",
    "\n",
    "\n",
    "## 贝叶斯网（Bayesian Networks）\n",
    "\n",
    "又称为信念网（Belief Networks）络或信度网络，有向图模型（Directed Graphical Models），形式上是一个有向无环图（Directed Acyclic Graph DAG）  \n",
    "一种基于概率推理的数学模型，理论基础是贝叶斯公式。  \n",
    "- 静态贝叶斯网（Static Bayesian Networks）\n",
    "- 动态贝叶斯网（Dynamic Bayesian Networks DBN）用于处理随时间变化的动态系统中的推断和预测问题。  \n",
    "  - **隐马尔科夫模型（Hidden Markov Model HMM）**应用在语音识别、汉语自动分词与词性标注和统计机器翻译等领域\n",
    "  - 卡尔曼滤波器（kalman Filter）应用在信号处理领域\n",
    "\n",
    "\n",
    "## 马尔科夫网（Markov Networks）\n",
    "\n",
    "\n",
    "又称为马尔科夫随机场（Markov Random Field，MRF），无向图模型（Undirected Graphical Models）  \n",
    "可视为随机的有限状态机（Finite State Machine FSM）。  \n",
    "描述了一类随机过程（随机函数，随时间而随机变化的过程）。  \n",
    "随着时间推移，从某一状态转移到另一状态。  \n",
    "- 条件随机场（Conditional Random Field CRF）应用于序列标注、特征选择、机器翻译等任务\n",
    "- 吉布斯/玻尔兹曼机（Gibbs/Boltzman Machine）应用于依存句法分析和语义角色标注\n",
    "  \n",
    "\n",
    "## 马尔科夫网 VS 贝叶斯网\n",
    "\n",
    "- 马尔科夫网可以表示贝叶斯网络无法表示的一些依赖关系，如循环依赖（环）\n",
    "- 马尔科夫网不能表示贝叶斯网络能够表示的某些关系，如推导关系\n",
    "\n",
    "**注意：隐马尔科夫模型是（动态）贝叶斯网络！！！**\n",
    "\n",
    "\n",
    "## 马尔科夫模型（Markov Model）\n",
    "\n",
    "马尔科夫模型描述了随时间而随机变化的随机过程（随机函数）。  \n",
    "在特定条件下，系统在时间 t 的状态只与其在时间 t-1 的状态相关，则该系统构成一个离散的一阶马尔科夫链（Markov Chain）。  \n",
    "如果只考虑独立于时间 t 的随机过程，该随机过程为马尔科夫模型。  \n",
    "在马尔科夫模型中，每个状态代表了一个可观察的事件，所以，马尔科夫模型有时又称作可视马尔科夫模型（visible Markov model, VMM）\n",
    "\n",
    "\n",
    "\n",
    "## 常见的图模型（Graphical Model）\n",
    "\n",
    "![](normal-graphs.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "## 自然语言处理（NLP）中概率图模型的演变\n",
    "\n",
    "![](pgm-develop.jpeg)\n",
    "\n",
    "\n",
    "## 判别式（Discriminative Model） VS 生成式（Generative Model）\n",
    "\n",
    "本质区别在于模型中观测序列和状态序列之间的决定关系。前者假设 y 决定 x，后者假设 x 决定 y。  \n",
    "- 生成式模型\n",
    "  - 『状态（输出）序列 y』按照一定的规律生成『观测（输入）x』序列为假设，针对联合分布进行建模，并且通过估计使生成概率最大的生成序列来获取 y\n",
    "  - 所有变量的全概率模型，因此可以模拟（『生成』）所有变量的值\n",
    "  - 在模型中一般都有严格的独立性假设，特征是事先给定的，并且特征之间的关系直接体现在公式中\n",
    "  - 优点：\n",
    "    - 处理单类问题时比较灵活\n",
    "    - 模型变量之间的关系比较清楚\n",
    "    - 模型可以通过增量学习获得\n",
    "    - 可用于数据不完整的情况\n",
    "  - 弱点：\n",
    "    - 模型的推导和学习比较复杂\n",
    "  - 典型模型：\n",
    "    - n 元语法模型（N-gram Model）\n",
    "    - HMM\n",
    "    - 朴素贝叶斯（Naive Bayes）分类器\n",
    "    - 概率上下文无关文法\n",
    "- 判别式模型\n",
    "  - 认为 y 由 x 决定，直接对后验概率 $p(y \\mid x)$ 进行建模，从 x 中提取特征，学习模型参数，使得条件概率符合一定形式的最优\n",
    "  - 特征可以任意给定，通过函数表示\n",
    "  - 优点：\n",
    "    - 处理多类问题或分辨某一类与其他类之间的差异时比较灵活，模型简单，容易建立和学习\n",
    "  - 弱点：\n",
    "    - 描述能力有限\n",
    "    - 变量之间的关系不清楚\n",
    "    - 绝大多数是有监督，不能扩展为无监督\n",
    "  - 典型模型：  \n",
    "    - 最大熵模型（Maximum-entropy Model）\n",
    "    - 条件随机场（Conditional Random Field CRF）\n",
    "    - 支持向量机（Support Vector Machine SVM）\n",
    "    - 最大熵马尔科夫模型（Maximum-entropy Markov Model MEMM）\n",
    "    - 感知机（Perceptron）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "结构最简单的动态贝叶斯网。  \n",
    "隐马尔科夫模型。\n",
    "\n",
    "## 定义\n",
    "\n",
    "![](hmm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第一组是状态变量：$\\{z_1,...,z_n\\}$，其中 $z_i \\in \\mathcal{Z}$ 表示第 $i$ 时刻的系统状态。\n",
    "  - 通常假定状态变量是隐藏的、不可观测的。\n",
    "  - 因此，状态变量也叫隐变量。\n",
    "- 第二组是观测变量：$\\{x_1,...,x_n\\}$，其中 $x_i \\in \\mathcal{X}$ 表示第 $i$ 时刻的观测值。\n",
    "- 在 HMM 中，系统通常在多个状态 $\\{s_1,...,s_n\\}$ 之间切换，因此状态变量 $z_i$ 的取值范围 $\\mathcal{Z}$（称为状态空间）通常是由 $N$ 个可能取值的离散空间。观测变量可以是离散型也可以是连续型。假定为离散型，则取值范围 $\\mathcal{X}$ 为 $\\{o_1,...o_n\\}$\n",
    "- 箭头表示依赖关系，任一时刻，观察变量的取值仅依赖于状态变量，即 $x_t$ 由 $z_t$ 确定，与其他状态变量及观测变量的取值无关。这就是『马尔科夫链』：系统下一时刻的状态仅由当前状态决定，不依赖于以往的任何状态。基于这种依赖关系，所有变量的联合概率分布为：  \n",
    "\n",
    "$$P(x_1,z_1,...,x_n,z_n) = P(z_1)P(x_1 \\mid z_1) \\prod_{i=2}^{n} P(z_i \\mid z_{i-1}) P(x_i \\mid z_i))$$\n",
    "\n",
    "\n",
    "- 举例来说：\n",
    "  - 应用到词性标注系统，词就是 x，词性就是 z\n",
    "  - 给定了一个词（o）+词性（s）的训练集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 假设\n",
    "\n",
    "- 齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻 t 的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关。\n",
    "  $$P(s_t \\mid s_{t-1}o_{t-1},...,s_1,o_1) = P(s_t \\mid s_{t-1})$$\n",
    "- 观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关。\n",
    "  $$P(o_t \\mid s_T o_T,...,s_t,o_t...,s_1,o_1) = P(o_t \\mid s_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数\n",
    "\n",
    "- 状态转移概率\n",
    "  - 模型在各个状态间转换的概率，通常记为矩阵 $A = [a_{ij}]_{N \\times N}$，其中   \n",
    "    $$a_{ij} = P(z_{t+1} = s_j \\mid z_t = s_i),\\quad 1 \\le i,j \\le N$$  \n",
    "    表示在任意时刻 $t$，若状态为 $s_i$，则在下一时刻状态为 $s_j$ 的概率\n",
    "\n",
    "- 输出观测概率\n",
    "  - 模型根据当前状态获得各个观测值的概率，通常记为矩阵 $B = [b_{ij}]_{N \\times M}$，其中  \n",
    "    $$b_{ij} = P(x_{t} = o_j \\mid z_t = s_i),\\quad 1 \\le i \\le N, \\  1 \\le j \\le M$$  \n",
    "    表示在任意时刻 $t$，若状态为 $s_i$，则观测值 $o_j$ 被获取的概率\n",
    "\n",
    "- 初始状态概率\n",
    "  - 模型在初始时刻各状态出现的概率，通常记为 $\\pi = (\\pi_1,...\\pi_N)$，其中  \n",
    "    $$\\pi_i = P(z_1 = s_i), \\quad 1 \\le i \\le N$$\n",
    "    表示初始状态为 $s_i$ 的概率\n",
    "\n",
    "通过指定状态空间 $\\mathcal{Z}$、观测空间 $\\mathcal{X}$ 和上述三组参数，就能确定一个隐马尔科夫模型。  \n",
    "参数 $\\lambda = [\\mathrm{A}, \\mathrm{B}, \\mathrm{\\pi}]$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How  \n",
    "\n",
    "## 过程\n",
    "\n",
    "给定隐马尔科夫模型的参数 $\\lambda$，它按如下过程产生观测序列 $\\{x_1,x_2,...,x_n\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置 $t=1$，并根据初始状态概率 $\\pi$ 选择初始状态 $z_1$\n",
    "- 根据状态 $z_t$ 和输出观测概率 $B$ 选择观测变量取值 $x_t$\n",
    "- 根据状态 $z_t$ 和状态转移矩阵 $A$ 转移模型状态，即确定 $z_{t+1}$\n",
    "- 若 $t < n$，设置 $t=t+1$，并转到第 2 步，否则停止\n",
    "\n",
    "其中，$y_t \\in \\{s_1,...,s_N\\}$ 和 $x_t \\in \\{o_1,...,o_N\\}$ 分别为第 $t$ 时刻的状态和观测值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本问题\n",
    "\n",
    "- 估计问题（概率计算问题）  \n",
    "  - 给定模型 $\\lambda = [\\mathrm{A}, \\mathrm{B}, \\mathrm{\\pi}]$，如何有效计算其产生观测序列 $x = \\{x_1,...,x_n\\}$ 的概率 $P(\\mathrm{x} \\mid \\lambda)$？  \n",
    "  - 即：如何评估模型与观测序列之间的匹配程度？  \n",
    "  - 如根据以往观测序列推测当前时刻最优可能的观测值\n",
    "\n",
    "- 训练问题（参数估计问题、学习问题）  \n",
    "  - 给定观测序列 $x = \\{x_1,...,x_n\\}$，如何调整模型参数 $\\lambda = [\\mathrm{A}, \\mathrm{B}, \\mathrm{\\pi}]$ 使得该序列出现的概率 $P(\\mathrm{x} \\mid \\lambda)$ 最大？  \n",
    "  - 即：如何训练模型使其能最好地描述观测数据？\n",
    "\n",
    "- 序列问题（预测问题）  \n",
    "  - 给定模型 $\\lambda = [\\mathrm{A}, \\mathrm{B}, \\mathrm{\\pi}]$，观测序列 $x = \\{x_1,...,x_n\\}$，如何找到与此观测序列最匹配的状态序列 $z = \\{z_1,...,z_n\\}$？  \n",
    "  - 即：如何根据观测序列推断出隐藏的模型状态？  \n",
    "  - 如语音识别中，观测值为语音信号，隐藏状态为文字，目标就是根据观测信号推断最有可能的状态序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设你是一个医生，眼前有个病人，你的任务是确定他是否得了感冒。\n",
    "  - 首先，病人的**状态**只有两种：{感冒，没有感冒}。\n",
    "  - 然后，病人的感觉（**观测**）有三种：{正常，冷，头晕}。\n",
    "  - 手头有病人的病例，你可以从病例的第一天确定 pi（初始状态概率向量）；\n",
    "  - 然后根据其他病例信息，确定 A（状态转移矩阵）也就是病人某天是否感冒和他第二天是否感冒的关系；\n",
    "  - 还可以确定 B（观测概率矩阵）也就是病人某天是什么状态和第二天他的感觉的关系。\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/machine_learning_basic/master/res/hmm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数和状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对应状态集合\n",
    "states = ('Healthy', 'Fever')\n",
    "# 对应观测集合\n",
    "obsers = ('normal', 'cold', 'dizzy')\n",
    "\n",
    "\n",
    "# 初始状态概率向量 π\n",
    "start_probability = {'Healthy': 0.6, 'Fever': 0.4}\n",
    "# 状态转移矩阵 A\n",
    "transition_probability = {'Healthy': {'Healthy': 0.7, 'Fever': 0.3},\n",
    "                          'Fever': {'Healthy': 0.4, 'Fever': 0.6},}\n",
    "# 观测概率矩阵（发射矩阵） B\n",
    "emission_probability = {\n",
    "    'Healthy': {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n",
    "    'Fever': {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6},}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机生成观测和状态序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_from(probs):\n",
    "    \"\"\"\n",
    "    1.np.random.multinomial:\n",
    "    按照多项式分布，生成数据\n",
    "    >>> np.random.multinomial(20, [1/6.]*6, size=2)\n",
    "            array([[3, 4, 3, 3, 4, 3],\n",
    "                   [2, 4, 3, 4, 0, 7]])\n",
    "     For the first run, we threw 3 times 1, 4 times 2, etc.  \n",
    "     For the second, we threw 2 times 1, 4 times 2, etc.\n",
    "    2.np.where:\n",
    "    >>> x = np.arange(9.).reshape(3, 3)\n",
    "    >>> np.where( x > 5 )\n",
    "    (array([2, 2, 2]), array([0, 1, 2]))\n",
    "    \"\"\"\n",
    "    # 1 次重复试验中，按 probs 概率分布下出现的次数，随机『自然』生成状态，分布和给定的 probs 一样\n",
    "    return np.where(np.random.multinomial(1, probs) == 1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机生成观测序列和状态序列    \n",
    "def simulate(t, A, B, probs):\n",
    "    observations = np.zeros(t, dtype=int)\n",
    "    states = np.zeros(t, dtype=int)\n",
    "    # 随机指定初始状态，根据初始状态概率分布\n",
    "    states[0] = draw_from(probs)\n",
    "    # 随机指定一个初始观测值，根据观测值状态概率分布\n",
    "    observations[0] = draw_from(B[states[0],:])\n",
    "    for _t in range(1, t):\n",
    "        # draw_from 返回的是位置\n",
    "        # states[0] 可能会变，A[index] 可能会变，所以 states[1] 也可能会变，进而 B[index] 可能会变，进而 observations[1] 会变\n",
    "        states[_t] = draw_from(A[states[_t-1],:])\n",
    "        observations[_t] = draw_from(B[states[_t],:])\n",
    "    return observations, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Healthy', 1: 'Fever'}\n",
      "{0: 'normal', 1: 'cold', 2: 'dizzy'}\n"
     ]
    }
   ],
   "source": [
    "states_index_label = dict(zip(range(len(states)), states))\n",
    "obsers_index_label = dict(zip(range(len(obsers)), obsers))\n",
    "\n",
    "print(states_index_label)\n",
    "print(obsers_index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[0.7 0.3]\n",
      " [0.4 0.6]]\n",
      "B: [[0.5 0.4 0.1]\n",
      " [0.1 0.3 0.6]]\n",
      "pi: [0.6 0.4]\n"
     ]
    }
   ],
   "source": [
    "A = pd.DataFrame(transition_probability, index=['Healthy', 'Fever'], columns=['Healthy', 'Fever']).T.values\n",
    "B = pd.DataFrame(emission_probability, index=['normal', 'cold', 'dizzy'], columns=['Healthy', 'Fever']).T.values\n",
    "pi = pd.DataFrame(start_probability, index=[None], columns=['Healthy', 'Fever']).values[0]\n",
    "print(\"A:\", A)\n",
    "print(\"B:\", B)\n",
    "print(\"pi:\", pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1 2 2 1 2]\n",
      "[0 1 0 1 1 1 1 1 1 1]\n",
      "['normal', 'normal', 'normal', 'cold', 'cold', 'cold', 'dizzy', 'dizzy', 'cold', 'dizzy']\n",
      "['Healthy', 'Fever', 'Healthy', 'Fever', 'Fever', 'Fever', 'Fever', 'Fever', 'Fever', 'Fever']\n"
     ]
    }
   ],
   "source": [
    "# 生成模拟数据\n",
    "observations_data, states_data = simulate(10, A, B, pi)\n",
    "print (observations_data)\n",
    "print (states_data)\n",
    "# 相应的label\n",
    "print ([obsers_index_label[index] for index in observations_data])\n",
    "print ([states_index_label[index] for index in states_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 估计问题\n",
    "\n",
    "\n",
    "给定一个观察序列 $x = \\{x_1,...,x_n\\}$ 和模型 $\\lambda = [\\mathrm{A}, \\mathrm{B}, \\mathrm{\\pi}]$，要快速地计算出给定模型 $\\lambda$ 情况下观察序列 $\\mathrm{x}$ 的概率，即： $P(\\mathrm{x} \\mid \\lambda)$。\n",
    "\n",
    "\n",
    "### 直接计算\n",
    "\n",
    "给定模型，求给定长度为 T 的观测序列的概率，直接计算法的思路是枚举所有的长度 T 的状态序列，计算该状态序列与观测序列的联合概率（隐状态发射到观测），对所有的枚举项求和即可。  \n",
    "在状态种类为 N 的情况下，一共有 $N^T$ 种排列组合，每种组合计算联合概率的计算量为 T，总的复杂度为 $O(TN^T)$，并不可取。   \n",
    "为此，人们提出了前向算法或前向计算过程（forward procedure），利用动态规划的方法来解决这一问题，将问题的时间复杂度降到 $O(TN^2)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向算法\n",
    "\n",
    "参考 [Forward algorithm - Wikipedia](https://en.wikipedia.org/wiki/Forward_algorithm) 即可，感觉比较清晰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(obs_seq, A, B, pi):\n",
    "    \"\"\"前向算法\"\"\"\n",
    "    N = A.shape[0]\n",
    "    T = len(obs_seq)\n",
    "    \n",
    "    # F对应α，F[n][t]=α_t(n)\n",
    "    F = np.zeros((N,T))\n",
    "    # P(S|O) ≈ P(O|S)P(S)\n",
    "    F[:,0] = pi * B[:, obs_seq[0]]\n",
    "\n",
    "    for t in range(1, T):\n",
    "        for n in range(N):\n",
    "            # 状态转移概率 * 发射概率，首先计算新的状态是什么\n",
    "            F[n,t] = np.dot(F[:,t-1], A[:,n]) * B[n, obs_seq[t]]\n",
    "\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward2(obs_seq, A, B, pi):\n",
    "    \"\"\"前向算法\"\"\"\n",
    "    N = A.shape[0]\n",
    "    T = len(obs_seq)\n",
    "    F = np.zeros((N,T))\n",
    "    F[:,0] = pi * B[:, obs_seq[0]]\n",
    "    for t in range(1, T):\n",
    "        F[:,t] = np.dot(F[:, t-1], A) * B[:, obs_seq[t]]\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.00000000e-01, 1.13000000e-01, 4.18300000e-02, 1.23642400e-02,\n",
       "        4.18167040e-03, 1.47845574e-03, 1.32387377e-04, 3.03143687e-05,\n",
       "        4.26088230e-05, 4.62718533e-06],\n",
       "       [4.00000000e-02, 1.14000000e-02, 4.07400000e-03, 4.49802000e-03,\n",
       "        1.92242520e-03, 7.22386872e-04, 5.26181308e-04, 2.13254999e-04,\n",
       "        4.11141929e-05, 2.24706976e-05]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  P(O_t+1 O_t+2 ... O_T | q_t = state_i , hmm)\n",
    "forward2(observations_data, A, B, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==\n"
     ]
    }
   ],
   "source": [
    "if forward(observations_data, A, B, pi).all() == forward2(observations_data, A, B, pi).all():\n",
    "    print(\"==\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后向算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(obs_seq, A, B, pi):\n",
    "    \"\"\"后向算法\"\"\"\n",
    "    N = A.shape[0]\n",
    "    T = len(obs_seq)\n",
    "\n",
    "    X = np.zeros((N,T))\n",
    "    X[:,-1:] = 1\n",
    "\n",
    "    for t in reversed(list(range(T-1))):\n",
    "        for n in range(N):\n",
    "            X[n,t] = np.sum(X[:,t+1] * A[n,:] * B[:, obs_seq[t+1]])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward2(obs_seq, A, B, pi):\n",
    "    \"\"\"后向算法\"\"\"\n",
    "    N = A.shape[0]\n",
    "    T = len(obs_seq)\n",
    "    X = np.zeros((N,T))\n",
    "    X[:,-1] = 1\n",
    "    for t in reversed(list(range(T-1))):\n",
    "        X[:,t] = np.sum(X[:,t+1] * A * B[:, obs_seq[t+1]], axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==\n"
     ]
    }
   ],
   "source": [
    "if backward(observations_data, A, B, pi).all() == backward2(observations_data, A, B, pi).all():\n",
    "    print(\"==\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习问题\n",
    "\n",
    "\n",
    "### 监督学习\n",
    "\n",
    "\n",
    "### Baum-Welch 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baum_welch_train(observations, criterion=0.005):\n",
    "    \"\"\"无监督学习算法——Baum-Weich算法\"\"\"\n",
    "    n_states = len(states)\n",
    "    n_obsers = len(obsers)\n",
    "    n_samples = len(observations)\n",
    "    A = np.array([1/n_states]*n_states*n_states).reshape(n_states, n_states)\n",
    "    B = np.array([1/n_obsers]*n_states*n_obsers).reshape(n_states, n_obsers)\n",
    "    pi = np.array([1/n_states]*n_states)\n",
    "    done = False\n",
    "    while not done:\n",
    "        # alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)\n",
    "        # Initialize alpha\n",
    "        alpha = forward(observations, A, B, pi)\n",
    "\n",
    "        # beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)\n",
    "        # Initialize beta\n",
    "        beta = backward(observations, A, B, pi)\n",
    "        # 计算ξ_t(i,j)=P(i_t=q_i,i_{i+1}=q_j|O,λ)\n",
    "        xi = np.zeros((n_states,n_states,n_samples-1))\n",
    "        for t in range(n_samples-1):\n",
    "            denom = np.dot(np.dot(alpha[:,t].T, A) * B[:,observations[t+1]].T, beta[:,t+1])\n",
    "            for i in range(n_states):\n",
    "                numer = alpha[i,t] * A[i,:] * B[:,observations[t+1]].T * beta[:,t+1].T\n",
    "                xi[i,:,t] = numer / denom\n",
    "\n",
    "        # γ_t(i)：gamma_t(i) = P(q_t = S_i | O, hmm)\n",
    "        gamma = np.sum(xi,axis=1)\n",
    "        # Need final gamma element for new B\n",
    "        # xi的第三维长度n_samples-1，少一个，所以gamma要计算最后一个\n",
    "        prod =  (alpha[:,n_samples-1] * beta[:,n_samples-1]).reshape((-1,1))\n",
    "        gamma = np.hstack((gamma,  prod / np.sum(prod))) #append one more to gamma!!!\n",
    "        \n",
    "        # 更新模型参数\n",
    "        newpi = gamma[:,0]\n",
    "        newA = np.sum(xi,2) / np.sum(gamma[:,:-1],axis=1).reshape((-1,1))\n",
    "        newB = np.copy(B)\n",
    "        num_levels = B.shape[1]\n",
    "        sumgamma = np.sum(gamma,axis=1)\n",
    "        for lev in range(num_levels):\n",
    "            mask = observations == lev\n",
    "            newB[:,lev] = np.sum(gamma[:,mask],axis=1) / sumgamma\n",
    "        \n",
    "        # 检查是否满足阈值\n",
    "        if np.max(abs(pi - newpi)) < criterion and \\\n",
    "                        np.max(abs(A - newA)) < criterion and \\\n",
    "                        np.max(abs(B - newB)) < criterion:\n",
    "            done = 1\n",
    "\n",
    "        A[:], B[:], pi[:] = newA, newB, newpi\n",
    "    return A, B, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.5, 0.5],\n",
       "        [0.5, 0.5]]), array([[0. , 0.6, 0.4],\n",
       "        [0. , 0.6, 0.4]]), array([0.5, 0.5]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baum_welch_train(observations_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测问题\n",
    "\n",
    "\n",
    "\n",
    "### 近似算法\n",
    "\n",
    "\n",
    "\n",
    "### 维特比算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(obs_seq, A, B, pi):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    V : numpy.ndarray\n",
    "        V [s][t] = Maximum probability of an observation sequence ending\n",
    "                   at time 't' with final state 's'\n",
    "    prev : numpy.ndarray\n",
    "        Contains a pointer to the previous state at t-1 that maximizes\n",
    "        V[state][t]\n",
    "        \n",
    "    V对应δ，prev对应ψ\n",
    "    \"\"\"\n",
    "    N = A.shape[0]\n",
    "    T = len(obs_seq)\n",
    "    prev = np.zeros((T - 1, N), dtype=int)\n",
    "\n",
    "    # DP matrix containing max likelihood of state at a given time\n",
    "    V = np.zeros((N, T))\n",
    "    V[:,0] = pi * B[:,obs_seq[0]]\n",
    "\n",
    "    for t in range(1, T):\n",
    "        for n in range(N):\n",
    "            seq_probs = V[:,t-1] * A[:,n] * B[n, obs_seq[t]]\n",
    "            prev[t-1,n] = np.argmax(seq_probs)\n",
    "            V[n,t] = np.max(seq_probs)\n",
    "\n",
    "    return V, prev\n",
    "\n",
    "def build_viterbi_path(prev, last_state):\n",
    "    \"\"\"Returns a state path ending in last_state in reverse order.\n",
    "    最优路径回溯\n",
    "    \"\"\"\n",
    "    T = len(prev)\n",
    "    yield(last_state)\n",
    "    for i in range(T-1, -1, -1):\n",
    "        yield(prev[i, last_state])\n",
    "        last_state = prev[i, last_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果  \n",
    "\n",
    "\n",
    "先建立一个 HMM 模型，再模拟出一个观测序列和一个状态序列。  \n",
    "然后，只用观测序列去学习模型，再去预测状态序列，观测准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_prob(obs_seq, A, B, pi):\n",
    "    \"\"\" P( entire observation sequence | A, B, pi ) \"\"\"\n",
    "    return np.sum(forward(obs_seq, A, B, pi)[:,-1])\n",
    "\n",
    "def state_path(obs_seq, A, B, pi):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    V[last_state, -1] : float\n",
    "        Probability of the optimal state path\n",
    "    path : list(int)\n",
    "        Optimal state path for the observation sequence\n",
    "    \"\"\"\n",
    "    V, prev = viterbi(obs_seq, A, B, pi)\n",
    "\n",
    "    # Build state path with greatest probability\n",
    "    last_state = np.argmax(V[:,-1])\n",
    "    path = list(build_viterbi_path(prev, last_state))\n",
    "\n",
    "    return V[last_state,-1], reversed(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.5, 0.5],[0.5, 0.5]])\n",
    "B = np.array([[0.3, 0.3, 0.3],[0.3, 0.3, 0.3]])\n",
    "pi = np.array([0.5, 0.5])\n",
    "\n",
    "observations_data, states_data = simulate(100, A, B, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42\n"
     ]
    }
   ],
   "source": [
    "A, B, pi = baum_welch_train(observations_data)\n",
    "states_out = state_path(observations_data, A, B, pi)[1]\n",
    "p = 0.0\n",
    "for s in states_data:\n",
    "    if next(states_out) == s:\n",
    "        p += 1\n",
    "\n",
    "print((p / len(states_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在假设诊所来了一位病人，他最近三天的病状是：  \n",
    "observations = ('normal', 'cold', 'dizzy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "obvers_index = list(obsers_index_label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            normal       cold      dizzy\n",
      "Healthy:   0.160000   0.021600   0.004428\n",
      "  Fever:   0.160000   0.021600   0.004428\n",
      "\n",
      "The most possible states and probability are:\n",
      "Healthy\n",
      "Healthy\n",
      "Healthy\n",
      "0.004428\n"
     ]
    }
   ],
   "source": [
    "V, p = viterbi(observations_index, A, B, pi)\n",
    "print(\" \" * 7, \" \".join((\"%10s\" % obsers_index_label[i]) for i in observations_index))\n",
    "for s in range(0, 2):\n",
    "    print((\"%7s: \" % states_index_label[s] + \" \".join(\"%10s\" % (\"%f\" % v) for v in V[s])))\n",
    "print ('\\nThe most possible states and probability are:')\n",
    "p, ss = state_path(observations_index, A, B, pi)\n",
    "for s in ss:\n",
    "    print((states_index_label[s]))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://ww3.sinaimg.cn/large/6cbb8645gw1f6m6ozfplig20dw07gadn.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 题外"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概率编程\n",
    "\n",
    "一种系统创建方法，帮助我们在面对不确定性时做出决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯学派 VS 频率学派"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "频率学派和贝叶斯学派最大的差别其实产生于对参数空间的认知上。所谓参数空间，就是你关心的那个参数可能的取值范围。  \n",
    "\n",
    "- 频率学派（其实就是当年的Fisher）\n",
    "  - 并不关心参数空间的所有细节，他们相信数据都是在这个空间里的”某个“参数值下产生的（虽然你不知道那个值是啥），所以他们的方法论一开始就是从“哪个值最有可能是真实值”这个角度出发的。于是就有了最大似然（maximum likelihood）以及置信区间（confidence interval）这样的东西，从名字就可以看出来他们关心的就是我有多大把握去圈出那个唯一的真实参数。\n",
    "  - 概率是事件在长事件内发生的频率\n",
    "  - 频率学派从「自然」角度出发，试图直接为「事件」本身建模，即事件 A 在独立重复试验中发生的频率趋于极限 p，那么这个极限就是该事件的概率\n",
    "- 贝叶斯学派\n",
    "  - 关心参数空间里的每一个值，因为觉得我们没有上帝视角，不可能知道哪个值是真的。所以参数空间里的每个值都有可能是真实模型使用的值，区别只是概率不同而已。于是才会引入先验分布（prior distribution）和后验分布（posterior distribution）这样的概念来设法找出参数空间上的每个值的概率。最好诠释这种差别的例子就是想象如果你的后验分布是双峰的，频率学派的方法会去选这两个峰当中较高的那一个对应的值作为他们的最好猜测，而贝叶斯学派则会同时报告这两个值，并给出对应的概率。\n",
    "  - 概率是对事件发生的信心，随证据更新信念\n",
    "  - 贝叶斯学派并不从试图刻画「事件」本身，而从「观察者」角度出发。贝叶斯学派并不试图说「事件本身是随机的」，或者「世界的本体带有某种随机性」，这套理论根本不言说关于「世界本体」的东西，而只是从「观察者知识不完备」这一出发点开始，构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法。频率学派下说的「随机事件」在贝叶斯学派看来，并不是「事件本身具有某种客观的随机性」，而是「观察者不知道事件的结果」而已，只是「观察者」知识状态中尚未包含这一事件的结果。但是在这种情况下，观察者又试图通过已经观察到的「证据」来推断这一事件的结果，因此只能靠猜。贝叶斯概率论就想构建一套比较完备的框架用来描述最能服务于理性推断这一目的的「猜的过程」。因此，在贝叶斯框架下，同一件事情对于知情者而言就是「确定事件」，对于不知情者而言就是「随机事件」，随机性并不源于事件本身是否发生，而只是描述观察者对该事件的知识状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "\n",
    "## 贝叶斯网 VS 马尔科夫网、马尔科夫模型\n",
    "\n",
    "- [贝叶斯网、马尔科夫随机场](http://blog.sciencenet.cn/blog-284987-474154.html)\n",
    "    - 本质上来讲，用 graphical model 解决一个问题，首先是要知道变量间的独立关系（independency among variables), 这样才便于建立图（I-Map)。  需要说明的是，无论是 bayesian network, 还是 MRF, 他们的定义是（P,G), 也就是同时定义概率和图，再 joint distribution  \n",
    "    - 严格 positive 的情况下，联合分布和图是等价的：图是联合分布的 I-Map,联合分布也能 factorize over the graph。\n",
    "    - 另外，对于贝叶斯图，概率分布在图上的 factorization 的定义很简单，而对于MRF，factoriztion的定义是：Joint distribution 中每个 factor 都是图 H 的 complete subgraph。注意了：complete subgraph 并不是 maximal clique，也就是，如果我们建立的图模型中，每三个变量组成一个 clique,这时，与这个图结构 compatible 的 joint distribution 完全可以是 pairwise 的 clique potentials 相乘而得到的。不过如果我们建立的图结构是每 3 个 variable 组成一个 clique, 我们会随之而建立 tri-order clique potentials 相乘的 gibbs distribution, 而不是 pairwise Markov models.  \n",
    "\n",
    "\n",
    "- [马尔可夫链的扩展贝叶斯网络](http://blog.sciencenet.cn/blog-215786-523456.html)\n",
    "    - 把有向图看成一个网络，它就是贝叶斯网络。其中每个圆圈表示一个状态。状态之间的连线表示它们的因果关系。比如从心血管疾病出发到吸烟的弧线表示心血管疾病可能和吸 有关。当然，这些关系可以有一个量化的可信度 (belief)，用一个概率描述。我们可以通过这样一张网络估计出一个人的心血管疾病的可能性。在网络中每个节点概率的计算，可以用贝叶斯公式来进行，贝叶斯网络因此而得名。由于网络的每个弧有一个可信度，贝叶斯网络也被称作信念网络 (belief networks)。\n",
    "    - 和马尔可夫链类似，贝叶斯网络中的每个状态值取决于前面有限个状态。不同的是，贝叶斯网络比马尔可夫链灵活，它不受马尔可夫链的链状结构的约束，因此可以更准确地描述事件之间的相关性。可以讲，马尔可夫链是贝叶斯网络的特例，而贝叶斯网络是马尔可夫链的推广。\n",
    "\n",
    "\n",
    "- [HMM和贝叶斯网络的关系是什么？](https://www.zhihu.com/question/37391215)\n",
    "  - 概念不同：HMM 模型中每个节点代表一个状态变量，状态变量产生观测变量；贝叶斯网络模型中一个节点可以有多个状态变量，每个状态对应一个状态值（例如概率）。 \n",
    "  - 目标不同：HMM 模型的目标通常是给出最有可能的结果，不关心其可信度，虽然它完全可以求出；贝叶斯的目标是给出每个结果及其置信度，从而为决策提供参考。  \n",
    "  - 相同点\n",
    "    - 都给前提条件：HMM 叫做观测变量序列（序列）；贝叶斯网络叫做证据变量集（集合）。\n",
    "    - 都提到隐变量：HMM 一定包含不可观测的隐变量，也就是观测变量背后的不可见的状态变量；贝叶斯网络可能包含隐变量（此时用梯度下降法），也可能不含隐变量（类似贝叶斯分类器）。有没有隐变量完全看给定的前提条件是否覆盖除输出之外的全部节点\n",
    "    - 都包含条件概率，有一定独立性：HMM 中当前状态只与前一状态有关，与其他时刻状态无关。状态随时间转移，当前观测变量由当前状态决定；贝叶斯网络中每一个节点受制于其父节点，独立于其他节点。没有时间序列的概念，节点间是逻辑上确定的因果关系。  \n",
    "  - 也就是说，HMM 模型是一条线从左到右计算出的。贝叶斯网络可以从任何一个点输入，从而得到所有点的输出。从这个角度看，有人形象地将贝叶斯网络模型比作马尔科夫链的非线性扩展。\n",
    "\n",
    "\n",
    "- [马尔可夫网络（马尔可夫随机场、无向图模型）](http://blog.csdn.net/zhuqiuhui/article/details/43267607)\n",
    "  - 用前苏联数学家辛钦（1894－1959〕的话来说，就是承认客观世界中有这样一种现象，其未来由现在决定的程度，使得我们关于过去的知识丝毫不影响这种决定性。这种在已知“现在”的条件下，“未来”与“过去”彼此独立的特性就被称为马尔科夫性，具有这种性质的随机过程就叫做马尔科夫过程，其最原始的模型就是马尔科夫链。这即是对荷兰数学家惠更斯（Ch. Huygens, 1629－1659）提出的无后效原理的概率推广，也是对法国数学家拉普拉斯（P. S. Laplace, 1749－1827）机械决定论的否定。\n",
    "  - 马尔可夫性质：  \n",
    "  它指的是一个随机变量序列按时间先后关系依次排开的时候，第N+1时刻的分布特性，与N时刻以前的随机变量的取值无关。拿天气来打个比方。如果我们假定天气是马尔可夫的，其意思就是我们假设今天的天气仅仅与昨天的天气存在概率上的关联，而与前天及前天以前的天气没有关系。其它如传染病和谣言的传播规律，就是马尔可夫的。 \n",
    "  - 随机场：  \n",
    "  当给每一个位置中按照某种分布随机赋予相空间的一个值之后，其全体就叫做随机场。我们不妨拿种地来打个比方。其中有两个概念：位置（site），相空间（phase space）。“位置”好比是一亩亩农田；“相空间”好比是种的各种庄稼。我们可以给不同的地种上不同的庄稼，这就好比给随机场的每个“位置”，赋予相空间里不同的值。所以，俗气点说，随机场就是在哪块地里种什么庄稼的事情。 \n",
    "  - 马尔可夫随机场：  \n",
    "    - 也叫马尔可夫网，拿种地打比方，如果任何一块地里种的庄稼的种类仅仅与它邻近的地里种的庄稼的种类有关，与其它地方的庄稼的种类无关，那么这些地里种的庄稼的集合，就是一个马尔可夫随机场。\n",
    "    - 无向图模型也叫马尔科夫随机场(Markov Random Fields)或马尔科夫网络(Markov Network)，无向图模型有一个简单的独立定义：两个节点集A、B都与给定的第三个节点集C相互条件独立，A、B节点之间的路径都被C中的节点分开。\n",
    "    - 相比之下，有向图模型也叫贝叶斯网络(Bayesian networks)或信念网络(Belief Networks)，有向图模型有一个更复杂的独立性观念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM\n",
    "\n",
    "- [统计自然语言处理（第2版） (豆瓣)](https://book.douban.com/subject/25746399/)\n",
    "- [隐马尔可夫模型-码农场](http://www.hankcs.com/ml/hidden-markov-model.html)\n",
    "- [Kevin Chan's blog - 隐马尔科夫模型（HMM）及其Python实现](https://applenob.github.io/hmm.html)\n",
    "- [Viterbi algorithm - Wikiwand](https://www.wikiwand.com/en/Viterbi_algorithm#/Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [贝叶斯学派与频率学派有何不同？ - 知乎](https://www.zhihu.com/question/20587681)\n",
    "- [统计学之边角料——频率派和贝叶斯派 | 桃子的博客铭](https://taozj.org/201604/statistics-frequency-bayesian.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
